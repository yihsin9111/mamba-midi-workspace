{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import package\n",
    "import numpy as np\n",
    "import os\n",
    "import collections\n",
    "import argparse\n",
    "# import librosa\n",
    "import random\n",
    "import torch\n",
    "# import heapq\n",
    "import glob\n",
    "import csv\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# import torchvision.models as models\n",
    "# from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('/mnt/gestalt/home/lonian/datasets/mamba_test_token/1.npy', allow_pickle=True)\n",
    "K, L = data.shape\n",
    "a = np.zeros((K*L))\n",
    "for i in range(K*L):\n",
    "    a[i] = data[i%4, i//4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, datalist) -> None:\n",
    "        self.data_path = datalist\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # shape = [1, 128, slice_length]\n",
    "        path = self.data_path[idx]\n",
    "        data = np.load(path, allow_pickle=True)\n",
    "        K, L = data.shape\n",
    "        a = np.zeros((K*L))\n",
    "        for i in range(K*L):\n",
    "            a[i] = data[i%4, i//4]\n",
    "        return \ttorch.LongTensor(a)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1746\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "path = glob('/mnt/gestalt/home/lonian/datasets/mamba_test_token/*.npy')\n",
    "# Dataset(path)\n",
    "print(len(path))\n",
    "train_data = Dataset(path)\n",
    "train_loader = DataLoader(dataset=train_data, batch_size = 8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lonian/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import MambaConfig, MambaModel\n",
    "from torch import nn\n",
    "class ScaledEmbedding(nn.Embedding):\n",
    "    \"\"\"Boost learning rate for embeddings (with `scale`).\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, lr=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.lr = lr\n",
    "\n",
    "    def make_optim_group(self):\n",
    "        group = {\"params\": list(self.parameters())}\n",
    "        if self.lr is not None:\n",
    "            group[\"lr\"] = self.lr\n",
    "        return group\n",
    "\n",
    "class Musicmamba(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Musicmamba, self).__init__()\n",
    "        # parameters setup\n",
    "        self.card = 2048\n",
    "        embed_dim = self.card\n",
    "        self.dim = 1024\n",
    "        self.token_layer = 4\n",
    "    \n",
    "        configuration = MambaConfig(\n",
    "            vocab_size = self.card,\n",
    "            hidden_size = self.dim\n",
    "        )\n",
    "        \n",
    "        self.emb = nn.ModuleList([ScaledEmbedding(embed_dim, self.dim) for _ in range(self.token_layer)])\n",
    "        # self.model = MambaModel(configuration)\n",
    "        # self.layers = []\n",
    "        # for _ in range(64):\n",
    "        #     self.layers.append(MambaModel(configuration).layers[0])\n",
    "        # self.layers = nn.ModuleList(self.layers)\n",
    "        self.layers = nn.ModuleList([MambaModel(configuration).layers[0] for _ in range(64)])\n",
    "        self.norm_f = MambaModel(configuration).norm_f\n",
    "        \n",
    "        self.linears = nn.ModuleList([nn.Linear(self.dim, self.card) for _ in range(self.token_layer)])\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        # out = self.model(x)\n",
    "        B, K, S = sequence.shape\n",
    "        input_ = sum([self.emb[k](sequence[:, k]) for k in range(K)])\n",
    "        for module in self.layers:\n",
    "            input_ = module(input_)\n",
    "            # print(out)\n",
    "        out = self.norm_f(input_)\n",
    "        logits = torch.stack([self.linears[k](out) for k in range(K)], dim=1)  # [B, K, S, card]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device = 'cpu'\n",
    "model = Musicmamba()\n",
    "# model = model.to(device)\n",
    "# data = np.load('/mnt/gestalt/home/lonian/datasets/mamba_test_token/1.npy', allow_pickle=True)\n",
    "# d = torch.LongTensor([data]).to(device)\n",
    "# output = model(d)\n",
    "# print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "method"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.modules of Musicmamba(\n",
      "  (emb): ModuleList(\n",
      "    (0-3): 4 x ScaledEmbedding(2048, 1024)\n",
      "  )\n",
      "  (layers): ModuleList(\n",
      "    (0-63): 64 x MambaBlock(\n",
      "      (norm): MambaRMSNorm()\n",
      "      (mixer): MambaMixer(\n",
      "        (conv1d): Conv1d(2048, 2048, kernel_size=(4,), stride=(1,), padding=(3,), groups=2048)\n",
      "        (act): SiLU()\n",
      "        (in_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "        (x_proj): Linear(in_features=2048, out_features=96, bias=False)\n",
      "        (dt_proj): Linear(in_features=64, out_features=2048, bias=True)\n",
      "        (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm_f): MambaRMSNorm()\n",
      "  (linears): ModuleList(\n",
      "    (0-3): 4 x Linear(in_features=1024, out_features=2048, bias=True)\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(str(model.modules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.get_submodule of Musicmamba(\n",
      "  (emb): ModuleList(\n",
      "    (0-3): 4 x ScaledEmbedding(2048, 1024)\n",
      "  )\n",
      "  (layers): ModuleList(\n",
      "    (0-63): 64 x MambaBlock(\n",
      "      (norm): MambaRMSNorm()\n",
      "      (mixer): MambaMixer(\n",
      "        (conv1d): Conv1d(2048, 2048, kernel_size=(4,), stride=(1,), padding=(3,), groups=2048)\n",
      "        (act): SiLU()\n",
      "        (in_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "        (x_proj): Linear(in_features=2048, out_features=96, bias=False)\n",
      "        (dt_proj): Linear(in_features=64, out_features=2048, bias=True)\n",
      "        (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm_f): MambaRMSNorm()\n",
      "  (linears): ModuleList(\n",
      "    (0-3): 4 x Linear(in_features=1024, out_features=2048, bias=True)\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(str(model.get_submodule))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1500)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.get_submodule of Musicmamba(\n",
      "  (emb): ModuleList(\n",
      "    (0-3): 4 x ScaledEmbedding(2048, 1024)\n",
      "  )\n",
      "  (layers_1): ModuleList(\n",
      "    (0-31): 32 x MambaBlock(\n",
      "      (norm): MambaRMSNorm()\n",
      "      (mixer): MambaMixer(\n",
      "        (conv1d): Conv1d(2048, 2048, kernel_size=(4,), stride=(1,), padding=(3,), groups=2048)\n",
      "        (act): SiLU()\n",
      "        (in_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "        (x_proj): Linear(in_features=2048, out_features=96, bias=False)\n",
      "        (dt_proj): Linear(in_features=64, out_features=2048, bias=True)\n",
      "        (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layers_2): ModuleList(\n",
      "    (0-31): 32 x MambaBlock(\n",
      "      (norm): MambaRMSNorm()\n",
      "      (mixer): MambaMixer(\n",
      "        (conv1d): Conv1d(2048, 2048, kernel_size=(4,), stride=(1,), padding=(3,), groups=2048)\n",
      "        (act): SiLU()\n",
      "        (in_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "        (x_proj): Linear(in_features=2048, out_features=96, bias=False)\n",
      "        (dt_proj): Linear(in_features=64, out_features=2048, bias=True)\n",
      "        (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm_f): MambaRMSNorm()\n",
      "  (linears): ModuleList(\n",
      "    (0-3): 4 x Linear(in_features=1024, out_features=2048, bias=True)\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "a =\"<bound method Module.get_submodule of Musicmamba(\\n  (emb): ModuleList(\\n    (0-3): 4 x ScaledEmbedding(2048, 1024)\\n  )\\n  (layers_1): ModuleList(\\n    (0-31): 32 x MambaBlock(\\n      (norm): MambaRMSNorm()\\n      (mixer): MambaMixer(\\n        (conv1d): Conv1d(2048, 2048, kernel_size=(4,), stride=(1,), padding=(3,), groups=2048)\\n        (act): SiLU()\\n        (in_proj): Linear(in_features=1024, out_features=4096, bias=False)\\n        (x_proj): Linear(in_features=2048, out_features=96, bias=False)\\n        (dt_proj): Linear(in_features=64, out_features=2048, bias=True)\\n        (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\\n      )\\n    )\\n  )\\n  (layers_2): ModuleList(\\n    (0-31): 32 x MambaBlock(\\n      (norm): MambaRMSNorm()\\n      (mixer): MambaMixer(\\n        (conv1d): Conv1d(2048, 2048, kernel_size=(4,), stride=(1,), padding=(3,), groups=2048)\\n        (act): SiLU()\\n        (in_proj): Linear(in_features=1024, out_features=4096, bias=False)\\n        (x_proj): Linear(in_features=2048, out_features=96, bias=False)\\n        (dt_proj): Linear(in_features=64, out_features=2048, bias=True)\\n        (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\\n      )\\n    )\\n  )\\n  (norm_f): MambaRMSNorm()\\n  (linears): ModuleList(\\n    (0-3): 4 x Linear(in_features=1024, out_features=2048, bias=True)\\n  )\\n)>\"\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 3/3 [06:07<00:00, 122.55s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"state-spaces/mamba-2.8b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaForCausalLM(\n",
       "  (backbone): MambaModel(\n",
       "    (embeddings): Embedding(50280, 2560)\n",
       "    (layers): ModuleList(\n",
       "      (0-63): 64 x MambaBlock(\n",
       "        (norm): MambaRMSNorm()\n",
       "        (mixer): MambaMixer(\n",
       "          (conv1d): Conv1d(5120, 5120, kernel_size=(4,), stride=(1,), padding=(3,), groups=5120)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "          (x_proj): Linear(in_features=5120, out_features=192, bias=False)\n",
       "          (dt_proj): Linear(in_features=160, out_features=5120, bias=True)\n",
       "          (out_proj): Linear(in_features=5120, out_features=2560, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_f): MambaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=50280, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n"
     ]
    }
   ],
   "source": [
    "append = np.array([[4], [3], [2], [1]])\n",
    "print(append.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1501)\n"
     ]
    }
   ],
   "source": [
    "prompt_seq = np.concatenate((data, append), axis=1)\n",
    "print(prompt_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 2, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_seq[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5996,)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0:-4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[-4:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5996,)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[4:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1500)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 778,  609,  609, ...,  289,  609,  289],\n",
       "       [ 911, 1758, 1815, ..., 2044, 1931, 2044],\n",
       "       [1747,  815, 1895, ..., 1509, 2021, 2003],\n",
       "       [1770, 1350, 1951, ...,  368, 1854, 1394]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MambaConfig, MambaModel\n",
    "\n",
    "# Initializing a Mamba configuration\n",
    "configuration = MambaConfig(\n",
    "    vocab_size = 2048,\n",
    "    hidden_size = 1024\n",
    ")\n",
    "\n",
    "# Initializing a model (with random weights) from the configuration\n",
    "model = MambaModel(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_params': 232.236032, 'total_trainable_params': 232.236032}\n"
     ]
    }
   ],
   "source": [
    "def cal_torch_model_params(model):\n",
    "    '''\n",
    "    :param model:\n",
    "    :return:\n",
    "    '''\n",
    "    # Find total parameters and trainable parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return {'total_params': total_params/1000000, 'total_trainable_params': total_trainable_params/1000000}\n",
    "\n",
    "print(cal_torch_model_params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaModel(\n",
       "  (embeddings): Embedding(2048, 128)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x MambaBlock(\n",
       "      (norm): MambaRMSNorm()\n",
       "      (mixer): MambaMixer(\n",
       "        (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)\n",
       "        (act): SiLU()\n",
       "        (in_proj): Linear(in_features=128, out_features=512, bias=False)\n",
       "        (x_proj): Linear(in_features=256, out_features=40, bias=False)\n",
       "        (dt_proj): Linear(in_features=8, out_features=256, bias=True)\n",
       "        (out_proj): Linear(in_features=256, out_features=128, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm_f): MambaRMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaConfig {\n",
       "  \"bos_token_id\": 0,\n",
       "  \"conv_kernel\": 4,\n",
       "  \"eos_token_id\": 0,\n",
       "  \"expand\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.1,\n",
       "  \"intermediate_size\": 1536,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"mamba\",\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"rescale_prenorm_residual\": false,\n",
       "  \"residual_in_fp32\": true,\n",
       "  \"state_size\": 16,\n",
       "  \"time_step_floor\": 0.0001,\n",
       "  \"time_step_init_scheme\": \"random\",\n",
       "  \"time_step_max\": 0.1,\n",
       "  \"time_step_min\": 0.001,\n",
       "  \"time_step_rank\": 48,\n",
       "  \"time_step_scale\": 1.0,\n",
       "  \"transformers_version\": \"4.41.2\",\n",
       "  \"use_bias\": false,\n",
       "  \"use_cache\": true,\n",
       "  \"use_conv_bias\": true,\n",
       "  \"vocab_size\": 50280\n",
       "}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaForCausalLM(\n",
       "  (backbone): MambaModel(\n",
       "    (embeddings): Embedding(50280, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x MambaBlock(\n",
       "        (norm): MambaRMSNorm()\n",
       "        (mixer): MambaMixer(\n",
       "          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (x_proj): Linear(in_features=1536, out_features=80, bias=False)\n",
       "          (dt_proj): Linear(in_features=48, out_features=1536, bias=True)\n",
       "          (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_f): MambaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50280, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import MultipleLocator\n",
    "version = 'text_v10'\n",
    "a = np.load('/mnt/gestalt/home/lonian/mamba/model/ckpts/{}/training_loss.npy'.format(version))\n",
    "# a = a/4\n",
    "\n",
    "#Example 1 Plot cos(x) between 0 and 2*pi\n",
    "x = range(1, int(len(a)+1)) # 50x1 array between 0 and 2*pi\n",
    "# y = np.cos(x)                  # cos(x)\n",
    "\n",
    "x_major_locator=MultipleLocator(50)\n",
    "y_major_locator=MultipleLocator(2)\n",
    "ax=plt.gca()\n",
    "ax.xaxis.set_major_locator(x_major_locator)\n",
    "ax.yaxis.set_major_locator(y_major_locator)\n",
    "# plt.xlim(0,200.5)\n",
    "plt.ylim(2,12)\n",
    "\n",
    "plt.plot(x, a, 'r')   # red line without marker\n",
    "plt.title('{} training loss curve'.format(version))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "# plt.show()\n",
    "# plt.plot(x, a, 'b-o') # blue solid line with filled circle marker\n",
    "plt.savefig('/mnt/gestalt/home/lonian/mamba/model/ckpts/{}/training_loss.png'.format(version))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \t 10.843802320805372\n",
      "2 \t 8.67422795440299\n",
      "3 \t 7.162676992381682\n",
      "4 \t 6.6798123815444255\n",
      "5 \t 6.400802315474279\n",
      "6 \t 6.205408804068933\n",
      "7 \t 6.053379916255586\n",
      "8 \t 5.928838266467528\n",
      "9 \t 5.826248675277553\n",
      "10 \t 5.743347564173367\n",
      "11 \t 5.671947523309941\n",
      "12 \t 5.60836360163946\n",
      "13 \t 5.552773988530664\n",
      "14 \t 5.503686438389733\n",
      "15 \t 5.4598516244791435\n",
      "16 \t 5.421443579285849\n",
      "17 \t 5.3862118587449075\n",
      "18 \t 5.354359612185135\n",
      "19 \t 5.324733899769601\n",
      "20 \t 5.297314636034269\n",
      "21 \t 5.271671598701494\n",
      "22 \t 5.247809229473984\n",
      "23 \t 5.225222253036759\n",
      "24 \t 5.203539178904943\n",
      "25 \t 5.183338814355736\n",
      "26 \t 5.1641546899329684\n",
      "27 \t 5.145740971881885\n",
      "28 \t 5.127981072740167\n",
      "29 \t 5.111010673517455\n",
      "30 \t 5.094686442826863\n",
      "31 \t 5.0788732587098355\n",
      "32 \t 5.0635885275931996\n",
      "33 \t 5.048985917019512\n",
      "34 \t 5.034665752552436\n",
      "35 \t 5.020727831280687\n",
      "36 \t 5.007200549287171\n",
      "37 \t 4.993606792427396\n",
      "38 \t 4.980864611790352\n",
      "39 \t 4.968111326200735\n",
      "40 \t 4.956007049942537\n",
      "41 \t 4.9435409280912594\n",
      "42 \t 4.931767256152203\n",
      "43 \t 4.919977108661589\n",
      "44 \t 4.908398424407943\n",
      "45 \t 4.896962381406868\n",
      "46 \t 4.885543066436699\n",
      "47 \t 4.87457641798341\n",
      "48 \t 4.863527774015581\n",
      "49 \t 4.852586255444405\n",
      "50 \t 4.841854089802146\n",
      "51 \t 4.8314420296034\n",
      "52 \t 4.820977607953179\n",
      "53 \t 4.8105596555605885\n",
      "54 \t 4.800144531813226\n",
      "55 \t 4.789888816142964\n",
      "56 \t 4.780132834534705\n",
      "57 \t 4.7698340478667784\n",
      "58 \t 4.760215011020369\n",
      "59 \t 4.750547127480793\n",
      "60 \t 4.7406456610555\n",
      "61 \t 4.730896537559838\n",
      "62 \t 4.721370149374442\n",
      "63 \t 4.711903700328293\n",
      "64 \t 4.702133563168125\n",
      "65 \t 4.692876045634487\n",
      "66 \t 4.683875110038777\n",
      "67 \t 4.674790038312111\n",
      "68 \t 4.665457996727843\n",
      "69 \t 4.656472204561448\n",
      "70 \t 4.647511617894459\n",
      "71 \t 4.638482295830659\n",
      "72 \t 4.629466188051543\n",
      "73 \t 4.620872459027028\n",
      "74 \t 4.612182421457269\n",
      "75 \t 4.603597985010135\n",
      "76 \t 4.59509386057164\n",
      "77 \t 4.586621980926642\n",
      "78 \t 4.578257774840419\n",
      "79 \t 4.5698430782162545\n",
      "80 \t 4.561689757212789\n",
      "81 \t 4.553695851723449\n",
      "82 \t 4.545557002340974\n",
      "83 \t 4.537426237809868\n",
      "84 \t 4.529645667374676\n",
      "85 \t 4.521971611885536\n",
      "86 \t 4.514196811172729\n",
      "87 \t 4.5065927150901555\n",
      "88 \t 4.498671947319338\n",
      "89 \t 4.491479187426674\n",
      "90 \t 4.483989096956589\n",
      "91 \t 4.476384203747737\n",
      "92 \t 4.4696329136900355\n",
      "93 \t 4.462071327349141\n",
      "94 \t 4.454978907484224\n",
      "95 \t 4.4480518626219725\n",
      "96 \t 4.441188831494532\n",
      "97 \t 4.434581842510681\n",
      "98 \t 4.427587194920598\n",
      "99 \t 4.420900507207131\n",
      "100 \t 4.4149577274258895\n",
      "101 \t 4.408534917589605\n",
      "102 \t 4.4018449084082105\n",
      "103 \t 4.3955769553879955\n",
      "104 \t 4.389304436232698\n",
      "105 \t 4.3834674277401025\n",
      "106 \t 4.377203081993713\n",
      "107 \t 4.371783744527977\n",
      "108 \t 4.3653129954602665\n",
      "109 \t 4.359872050651136\n",
      "110 \t 4.354748759723707\n",
      "111 \t 4.349078454199236\n",
      "112 \t 4.343171738562546\n",
      "113 \t 4.338319660624712\n",
      "114 \t 4.333079020694359\n",
      "115 \t 4.327736582157466\n",
      "116 \t 4.322179911026743\n",
      "117 \t 4.317318195995596\n",
      "118 \t 4.312795569213178\n",
      "119 \t 4.308047779776459\n",
      "120 \t 4.303305146105366\n",
      "121 \t 4.298753392902412\n",
      "122 \t 4.294301582609906\n",
      "123 \t 4.289138824795867\n",
      "124 \t 4.285232770191748\n",
      "125 \t 4.281033602264738\n",
      "126 \t 4.276661311490309\n",
      "127 \t 4.2726322031151245\n",
      "128 \t 4.268558129370032\n",
      "129 \t 4.264805976859434\n",
      "130 \t 4.2611027296255255\n",
      "131 \t 4.257366662227146\n",
      "132 \t 4.253654794901917\n",
      "133 \t 4.24979085501864\n",
      "134 \t 4.246612302123091\n",
      "135 \t 4.243046766559307\n",
      "136 \t 4.23978213032424\n",
      "137 \t 4.236605125983749\n",
      "138 \t 4.233308379454205\n",
      "139 \t 4.230570207298561\n",
      "140 \t 4.227274340714665\n",
      "141 \t 4.224499441805285\n",
      "142 \t 4.221522453935165\n",
      "143 \t 4.218896908045104\n",
      "144 \t 4.216208284663171\n",
      "145 \t 4.213629134030976\n",
      "146 \t 4.211053689875119\n",
      "147 \t 4.208689223181992\n",
      "148 \t 4.206490579592669\n",
      "149 \t 4.203924675198162\n",
      "150 \t 4.201609634554263\n",
      "151 \t 4.19997254436019\n",
      "152 \t 4.197598940173809\n",
      "153 \t 4.195588659479301\n",
      "154 \t 4.193586215801568\n",
      "155 \t 4.191716861695936\n",
      "156 \t 4.190054502593451\n",
      "157 \t 4.188364741403165\n",
      "158 \t 4.186398716524849\n",
      "159 \t 4.1853771949443335\n",
      "160 \t 4.183537407001631\n",
      "161 \t 4.181738656323704\n",
      "162 \t 4.1804332300035645\n",
      "163 \t 4.178936438904957\n",
      "164 \t 4.177841124858475\n",
      "165 \t 4.176761242920807\n",
      "166 \t 4.175289459250926\n",
      "167 \t 4.173989524347555\n",
      "168 \t 4.173036613116993\n",
      "169 \t 4.171711573823428\n",
      "170 \t 4.171069833484471\n",
      "171 \t 4.170157527963345\n",
      "172 \t 4.169368906833317\n",
      "173 \t 4.168824781037374\n",
      "174 \t 4.167336760487537\n",
      "175 \t 4.166702417016318\n",
      "176 \t 4.166641627173484\n",
      "177 \t 4.165849576643772\n",
      "178 \t 4.165175695512207\n",
      "179 \t 4.164439329781483\n",
      "180 \t 4.163915198904012\n",
      "181 \t 4.163867689492921\n",
      "182 \t 4.163248069841909\n",
      "183 \t 4.162486089177751\n",
      "184 \t 4.162244710996702\n",
      "185 \t 4.1623790990934\n",
      "186 \t 4.161770919359331\n",
      "187 \t 4.161417644247277\n",
      "188 \t 4.161541482945368\n",
      "189 \t 4.160932982325771\n",
      "190 \t 4.160658581102162\n",
      "191 \t 4.160362520394649\n",
      "192 \t 4.160275568947277\n",
      "193 \t 4.15995928496032\n",
      "194 \t 4.159779828468766\n",
      "195 \t 4.160018232896734\n",
      "196 \t 4.159660910596769\n",
      "197 \t 4.159651469911425\n",
      "198 \t 4.160015567965476\n",
      "199 \t 4.159649325120153\n",
      "200 \t 4.159620924925645\n"
     ]
    }
   ],
   "source": [
    "for idx, aa in enumerate(a):\n",
    "    print(idx+1, '\\t', aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16.51710469, 11.21570262,  7.90007398,  7.17649954,  6.82042663,\n",
       "        6.57737762,  6.4055388 ,  6.28017034,  6.17770326,  6.08869242,\n",
       "        6.00751681,  5.93016965,  5.86258433,  5.80313026,  5.74896246,\n",
       "        5.70094316,  5.65616513,  5.61368549,  5.57458321,  5.53725072,\n",
       "        5.5017525 ,  5.46727222,  5.4338563 ,  5.40175269,  5.37063075,\n",
       "        5.34136607,  5.31349127,  5.28703623,  5.26071534,  5.23604451,\n",
       "        5.2121158 ,  5.18885966,  5.16776611,  5.14709215,  5.12755379,\n",
       "        5.10880421,  5.09157544,  5.0744506 ,  5.05860186,  5.04306475,\n",
       "        5.02840587,  5.01438547,  5.00111889,  4.98838885,  4.97599844,\n",
       "        4.96390166,  4.95304892,  4.94146319,  4.93132942,  4.92073443,\n",
       "        4.91115333,  4.90208731,  4.89247134,  4.88356306,  4.87501706,\n",
       "        4.86657865,  4.85873804,  4.85045628,  4.84309226,  4.83519033,\n",
       "        4.82857988,  4.82132158,  4.81396425,  4.80823474,  4.80141706,\n",
       "        4.79496466,  4.78906058,  4.78314414,  4.77738829,  4.77148051,\n",
       "        4.76594986,  4.7605493 ,  4.75529067,  4.75024984,  4.74503159,\n",
       "        4.74034945,  4.73545502,  4.73069172,  4.72624295,  4.72160338,\n",
       "        4.71697419,  4.71297348,  4.70872468,  4.70464878,  4.70035578,\n",
       "        4.69626572])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([16.51710469, 11.21570262,  7.90007398,  7.17649954,  6.82042663,\n",
    "        6.57737762,  6.4055388 ,  6.28017034,  6.17770326,  6.08869242,\n",
    "        6.00751681,  5.93016965,  5.86258433,  5.80313026,  5.74896246,\n",
    "        5.70094316,  5.65616513,  5.61368549,  5.57458321,  5.53725072,\n",
    "        5.5017525 ,  5.46727222,  5.4338563 ,  5.40175269,  5.37063075,\n",
    "        5.34136607,  5.31349127,  5.28703623,  5.26071534,  5.23604451,\n",
    "        5.2121158 ,  5.18885966,  5.16776611,  5.14709215,  5.12755379,\n",
    "        5.10880421,  5.09157544,  5.0744506 ,  5.05860186,  5.04306475,\n",
    "        5.02840587,  5.01438547,  5.00111889,  4.98838885,\n",
    "        4.97599844,  4.96390166,  4.95304892,  4.94146319,  4.93132942,\n",
    "        4.92073443,  4.91115333,  4.90208731,  4.89247134,  4.88356306,\n",
    "        4.87501706,  4.86657865,  4.85873804,  4.85045628,  4.84309226,\n",
    "        4.83519033,  4.82857988,  4.82132158,  4.81396425,  4.80823474,\n",
    "        4.80141706,  4.79496466,  4.78906058,  4.78314414,  4.77738829,\n",
    "        4.77148051,  4.76594986,  4.7605493 ,  4.75529067,  4.75024984,\n",
    "        4.74503159,  4.74034945,  4.73545502,  4.73069172,  4.72624295,\n",
    "        4.72160338,  4.71697419,  4.71297348,  4.70872468,  4.70464878,\n",
    "        4.70035578,  4.69626572])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "checkpoints_path = '/mnt/gestalt/home/lonian/mamba/model/ckpts/v18/epoch_042.pkl'\n",
    "if os.path.isfile(checkpoints_path):\n",
    "    checkpoint = torch.load(checkpoints_path, map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.014385472116688"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mamba_ssm import Mamba2\n",
    "\n",
    "batch, length, dim = 2, 64, 1024\n",
    "\n",
    "x = torch.randn(batch, length, dim).to(\"cuda\")\n",
    "\n",
    "model = Mamba2(\n",
    "    # This module uses roughly 3 * expand * d_model^2 parameters\n",
    "    d_model=1024, # Model dimension d_model\n",
    "    d_state=64,  # SSM state expansion factor, typically 64 or 128\n",
    "    d_conv=4,    # Local convolution width\n",
    "    expand=2,    # Block expansion factor\n",
    ").to(\"cuda\")\n",
    "y = model(x)\n",
    "assert y.shape == x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mamba_ssm.ops.triton.layernorm_gated import RMSNorm as RMSNormGated, LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mamba_ssm.models.config_mamba import MambaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MambaConfig(d_model=20, d_intermediate=0, n_layer=64, vocab_size=50277, ssm_cfg={}, attn_layer_idx=[], attn_cfg={}, rms_norm=True, residual_in_fp32=True, fused_add_norm=True, pad_vocab_size_multiple=8, tie_embeddings=True)\n"
     ]
    }
   ],
   "source": [
    "a = MambaConfig()\n",
    "a.d_model = 20\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {\"layer\": \"Mamba2\"}\n",
    "b = a.pop(\"layer\", \"Mamba1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mamba2'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lonian/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1500]) torch.Size([4, 1500, 1024])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Pointer argument (at 0) cannot be accessed from Triton (cpu tensor?)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2048\u001b[39m,(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m1500\u001b[39m))\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# x = torch.LongTensor(x)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m y \u001b[38;5;241m=\u001b[39m music_model(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/gestalt/home/lonian/mamba/model/models.py:292\u001b[0m, in \u001b[0;36mMambaLMHeadModel.forward\u001b[0;34m(self, input_ids, position_ids, inference_params, num_last_tokens, **mixer_kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, position_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, inference_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, num_last_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmixer_kwargs):\n\u001b[1;32m    288\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;03m    \"position_ids\" is just to be compatible with Transformer generation. We don't use it.\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    num_last_tokens: if > 0, only return the logits for the last n tokens\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone(input_ids, inference_params\u001b[38;5;241m=\u001b[39minference_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmixer_kwargs)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_last_tokens \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    294\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m hidden_states[:, \u001b[38;5;241m-\u001b[39mnum_last_tokens:]\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/gestalt/home/lonian/mamba/model/models.py:207\u001b[0m, in \u001b[0;36mMixerModel.forward\u001b[0;34m(self, input_ids, inference_params, **mixer_kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 207\u001b[0m     hidden_states, residual \u001b[38;5;241m=\u001b[39m layer(\n\u001b[1;32m    208\u001b[0m         hidden_states, residual, inference_params\u001b[38;5;241m=\u001b[39minference_params\n\u001b[1;32m    209\u001b[0m     )\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfused_add_norm:\n\u001b[1;32m    211\u001b[0m     residual \u001b[38;5;241m=\u001b[39m (hidden_states \u001b[38;5;241m+\u001b[39m residual) \u001b[38;5;28;01mif\u001b[39;00m residual \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/mamba_ssm/modules/block.py:57\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, hidden_states, residual, inference_params, **mixer_kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m         residual \u001b[38;5;241m=\u001b[39m residual\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     hidden_states, residual \u001b[38;5;241m=\u001b[39m layer_norm_fn(\n\u001b[1;32m     58\u001b[0m         hidden_states,\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m     61\u001b[0m         residual\u001b[38;5;241m=\u001b[39mresidual,\n\u001b[1;32m     62\u001b[0m         prenorm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     63\u001b[0m         residual_in_fp32\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_in_fp32,\n\u001b[1;32m     64\u001b[0m         eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm\u001b[38;5;241m.\u001b[39meps,\n\u001b[1;32m     65\u001b[0m         is_rms_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm, RMSNorm)\n\u001b[1;32m     66\u001b[0m     )\n\u001b[1;32m     67\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmixer(hidden_states, inference_params\u001b[38;5;241m=\u001b[39minference_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmixer_kwargs)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/mamba_ssm/ops/triton/layer_norm.py:875\u001b[0m, in \u001b[0;36mlayer_norm_fn\u001b[0;34m(x, weight, bias, residual, x1, weight1, bias1, eps, dropout_p, rowscale, prenorm, residual_in_fp32, is_rms_norm, return_dropout_mask)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlayer_norm_fn\u001b[39m(\n\u001b[1;32m    860\u001b[0m     x,\n\u001b[1;32m    861\u001b[0m     weight,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    873\u001b[0m     return_dropout_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    874\u001b[0m ):\n\u001b[0;32m--> 875\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LayerNormFn\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m    876\u001b[0m         x,\n\u001b[1;32m    877\u001b[0m         weight,\n\u001b[1;32m    878\u001b[0m         bias,\n\u001b[1;32m    879\u001b[0m         residual,\n\u001b[1;32m    880\u001b[0m         x1,\n\u001b[1;32m    881\u001b[0m         weight1,\n\u001b[1;32m    882\u001b[0m         bias1,\n\u001b[1;32m    883\u001b[0m         eps,\n\u001b[1;32m    884\u001b[0m         dropout_p,\n\u001b[1;32m    885\u001b[0m         rowscale,\n\u001b[1;32m    886\u001b[0m         prenorm,\n\u001b[1;32m    887\u001b[0m         residual_in_fp32,\n\u001b[1;32m    888\u001b[0m         is_rms_norm,\n\u001b[1;32m    889\u001b[0m         return_dropout_mask,\n\u001b[1;32m    890\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/torch/autograd/function.py:598\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    606\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/mamba_ssm/ops/triton/layer_norm.py:748\u001b[0m, in \u001b[0;36mLayerNormFn.forward\u001b[0;34m(ctx, x, weight, bias, residual, x1, weight1, bias1, eps, dropout_p, rowscale, prenorm, residual_in_fp32, is_rms_norm, return_dropout_mask)\u001b[0m\n\u001b[1;32m    742\u001b[0m     rowscale \u001b[38;5;241m=\u001b[39m rowscale\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    743\u001b[0m residual_dtype \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    744\u001b[0m     residual\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    745\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m residual \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    746\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39mfloat32 \u001b[38;5;28;01mif\u001b[39;00m residual_in_fp32 \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    747\u001b[0m )\n\u001b[0;32m--> 748\u001b[0m y, y1, mean, rstd, residual_out, seeds, dropout_mask, dropout_mask1 \u001b[38;5;241m=\u001b[39m _layer_norm_fwd(\n\u001b[1;32m    749\u001b[0m     x,\n\u001b[1;32m    750\u001b[0m     weight,\n\u001b[1;32m    751\u001b[0m     bias,\n\u001b[1;32m    752\u001b[0m     eps,\n\u001b[1;32m    753\u001b[0m     residual,\n\u001b[1;32m    754\u001b[0m     x1,\n\u001b[1;32m    755\u001b[0m     weight1,\n\u001b[1;32m    756\u001b[0m     bias1,\n\u001b[1;32m    757\u001b[0m     dropout_p\u001b[38;5;241m=\u001b[39mdropout_p,\n\u001b[1;32m    758\u001b[0m     rowscale\u001b[38;5;241m=\u001b[39mrowscale,\n\u001b[1;32m    759\u001b[0m     residual_dtype\u001b[38;5;241m=\u001b[39mresidual_dtype,\n\u001b[1;32m    760\u001b[0m     is_rms_norm\u001b[38;5;241m=\u001b[39mis_rms_norm,\n\u001b[1;32m    761\u001b[0m     return_dropout_mask\u001b[38;5;241m=\u001b[39mreturn_dropout_mask,\n\u001b[1;32m    762\u001b[0m )\n\u001b[1;32m    763\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\n\u001b[1;32m    764\u001b[0m     residual_out, weight, bias, weight1, bias1, rowscale, seeds, mean, rstd\n\u001b[1;32m    765\u001b[0m )\n\u001b[1;32m    766\u001b[0m ctx\u001b[38;5;241m.\u001b[39mx_shape_og \u001b[38;5;241m=\u001b[39m x_shape_og\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/mamba_ssm/ops/triton/layer_norm.py:335\u001b[0m, in \u001b[0;36m_layer_norm_fwd\u001b[0;34m(x, weight, bias, eps, residual, x1, weight1, bias1, dropout_p, rowscale, out_dtype, residual_dtype, is_rms_norm, return_dropout_mask)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis layer norm doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt support feature dim >= 64KB.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(x\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mindex):\n\u001b[0;32m--> 335\u001b[0m     _layer_norm_fwd_1pass_kernel[(M,)](\n\u001b[1;32m    336\u001b[0m         x,\n\u001b[1;32m    337\u001b[0m         y,\n\u001b[1;32m    338\u001b[0m         weight,\n\u001b[1;32m    339\u001b[0m         bias,\n\u001b[1;32m    340\u001b[0m         residual,\n\u001b[1;32m    341\u001b[0m         x1,\n\u001b[1;32m    342\u001b[0m         weight1,\n\u001b[1;32m    343\u001b[0m         bias1,\n\u001b[1;32m    344\u001b[0m         y1,\n\u001b[1;32m    345\u001b[0m         residual_out,\n\u001b[1;32m    346\u001b[0m         rowscale,\n\u001b[1;32m    347\u001b[0m         seeds,\n\u001b[1;32m    348\u001b[0m         dropout_mask,\n\u001b[1;32m    349\u001b[0m         mean,\n\u001b[1;32m    350\u001b[0m         rstd,\n\u001b[1;32m    351\u001b[0m         x\u001b[38;5;241m.\u001b[39mstride(\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m    352\u001b[0m         y\u001b[38;5;241m.\u001b[39mstride(\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m    353\u001b[0m         residual\u001b[38;5;241m.\u001b[39mstride(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m residual \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    354\u001b[0m         residual_out\u001b[38;5;241m.\u001b[39mstride(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m residual_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    355\u001b[0m         x1\u001b[38;5;241m.\u001b[39mstride(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m x1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    356\u001b[0m         y1\u001b[38;5;241m.\u001b[39mstride(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m y1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    357\u001b[0m         M,\n\u001b[1;32m    358\u001b[0m         N,\n\u001b[1;32m    359\u001b[0m         eps,\n\u001b[1;32m    360\u001b[0m         dropout_p,\n\u001b[1;32m    361\u001b[0m         is_rms_norm,\n\u001b[1;32m    362\u001b[0m         BLOCK_N,\n\u001b[1;32m    363\u001b[0m         residual \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    364\u001b[0m         residual_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    365\u001b[0m         bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    366\u001b[0m         dropout_p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m    367\u001b[0m         dropout_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    368\u001b[0m         rowscale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    369\u001b[0m     )\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# residual_out is None if residual is None and residual_dtype == input_dtype and dropout_p == 0.0\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dropout_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m x1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/triton/runtime/jit.py:167\u001b[0m, in \u001b[0;36mKernelInterface.__getitem__.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, grid) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    A JIT function is launched with: fn[grid](*args, **kwargs).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    Hence JITFunction.__getitem__ returns a callable proxy that\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m    memorizes the grid.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun(grid\u001b[38;5;241m=\u001b[39mgrid, warmup\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/triton/runtime/autotuner.py:143\u001b[0m, in \u001b[0;36mAutotuner.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m pruned_configs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprune_configs(kwargs)\n\u001b[1;32m    142\u001b[0m bench_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 143\u001b[0m timings \u001b[38;5;241m=\u001b[39m {config: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bench(\u001b[38;5;241m*\u001b[39margs, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m pruned_configs}\n\u001b[1;32m    144\u001b[0m bench_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbench_time \u001b[38;5;241m=\u001b[39m bench_end \u001b[38;5;241m-\u001b[39m bench_start\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/triton/runtime/autotuner.py:122\u001b[0m, in \u001b[0;36mAutotuner._bench\u001b[0;34m(self, config, *args, **meta)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_hook(args)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m do_bench(kernel_call, warmup\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarmup, rep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrep, quantiles\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.8\u001b[39m))\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutOfResources:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/triton/testing.py:102\u001b[0m, in \u001b[0;36mdo_bench\u001b[0;34m(fn, warmup, rep, grad_to_none, quantiles, fast_flush, return_mode)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03mBenchmark the runtime of the provided function. By default, return the median runtime of :code:`fn` along with\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03mthe 20-th and 80-th performance percentile.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m:type fast_flush: bool\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m fn()\n\u001b[1;32m    103\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# We maintain a buffer of 256 MB that we clear\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# before each kernel call to make sure that the L2\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# doesn't contain any input data before the run\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/triton/runtime/autotuner.py:110\u001b[0m, in \u001b[0;36mAutotuner._bench.<locals>.kernel_call\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m     config\u001b[38;5;241m.\u001b[39mpre_hook(full_nargs)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_hook(args)\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m    112\u001b[0m     num_warps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_warps,\n\u001b[1;32m    113\u001b[0m     num_stages\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_stages,\n\u001b[1;32m    114\u001b[0m     num_ctas\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_ctas,\n\u001b[1;32m    115\u001b[0m     enable_warp_specialization\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39menable_warp_specialization,\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# enable_persistent=False,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcurrent,\n\u001b[1;32m    118\u001b[0m )\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_hook(args)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/triton/runtime/autotuner.py:305\u001b[0m, in \u001b[0;36mHeuristics.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v, heur \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    304\u001b[0m     kwargs[v] \u001b[38;5;241m=\u001b[39m heur({\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marg_names, args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs})\n\u001b[0;32m--> 305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/triton/runtime/autotuner.py:305\u001b[0m, in \u001b[0;36mHeuristics.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v, heur \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    304\u001b[0m     kwargs[v] \u001b[38;5;241m=\u001b[39m heur({\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marg_names, args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs})\n\u001b[0;32m--> 305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/triton/runtime/autotuner.py:305\u001b[0m, in \u001b[0;36mHeuristics.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v, heur \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    304\u001b[0m     kwargs[v] \u001b[38;5;241m=\u001b[39m heur({\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marg_names, args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs})\n\u001b[0;32m--> 305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/triton/runtime/jit.py:425\u001b[0m, in \u001b[0;36mJITFunction.run\u001b[0;34m(self, grid, warmup, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m warmup:\n\u001b[1;32m    424\u001b[0m     args \u001b[38;5;241m=\u001b[39m [arg\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m arg\u001b[38;5;241m.\u001b[39mparam\u001b[38;5;241m.\u001b[39mis_constexpr]\n\u001b[0;32m--> 425\u001b[0m     kernel\u001b[38;5;241m.\u001b[39mrun(grid_0, grid_1, grid_2, kernel\u001b[38;5;241m.\u001b[39mnum_warps, kernel\u001b[38;5;241m.\u001b[39mnum_ctas,  \u001b[38;5;66;03m# number of warps/ctas per instance\u001b[39;00m\n\u001b[1;32m    426\u001b[0m                kernel\u001b[38;5;241m.\u001b[39mcluster_dims[\u001b[38;5;241m0\u001b[39m], kernel\u001b[38;5;241m.\u001b[39mcluster_dims[\u001b[38;5;241m1\u001b[39m], kernel\u001b[38;5;241m.\u001b[39mcluster_dims[\u001b[38;5;241m2\u001b[39m],  \u001b[38;5;66;03m# cluster\u001b[39;00m\n\u001b[1;32m    427\u001b[0m                kernel\u001b[38;5;241m.\u001b[39mshared, stream, kernel\u001b[38;5;241m.\u001b[39mfunction, CompiledKernel\u001b[38;5;241m.\u001b[39mlaunch_enter_hook,\n\u001b[1;32m    428\u001b[0m                CompiledKernel\u001b[38;5;241m.\u001b[39mlaunch_exit_hook, kernel,\n\u001b[1;32m    429\u001b[0m                \u001b[38;5;241m*\u001b[39mdriver\u001b[38;5;241m.\u001b[39massemble_tensormap_to_arg(kernel\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensormaps_info\u001b[39m\u001b[38;5;124m\"\u001b[39m], args))\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m kernel\n",
      "\u001b[0;31mValueError\u001b[0m: Pointer argument (at 0) cannot be accessed from Triton (cpu tensor?)"
     ]
    }
   ],
   "source": [
    "from models import MambaLMHeadModel\n",
    "from mamba_ssm.models.config_mamba import MambaConfig\n",
    "import torch\n",
    "# model config setup\n",
    "model_config = MambaConfig()\n",
    "model_config.n_layer = 48\n",
    "model_config.attn_layer_idx = [11, 23, 35, 47]\n",
    "model_config.attn_cfg = {'num_heads': 8}\n",
    "model_config.vocab_size = 2048\n",
    "model_config.d_model = 1024\n",
    "model_config.ssm_cfg = {\"layer\": \"Mamba2\", \"d_state\":256}\n",
    "\n",
    "music_model = MambaLMHeadModel(model_config)\n",
    "x = torch.randint(0, 2048,(4, 1500))\n",
    "# x = torch.LongTensor(x)\n",
    "y = music_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mamba_ssm import Mamba2\n",
    "import torch\n",
    "batch, length, dim = 2, 64, 512\n",
    "x = torch.randn(batch, length, dim).to(\"cuda\")\n",
    "\n",
    "model = Mamba2(\n",
    "    # This module uses roughly 3 * expand * d_model^2 parameters\n",
    "    d_model=dim, # Model dimension d_model\n",
    "    d_state=64,  # SSM state expansion factor, typically 64 or 128\n",
    "    d_conv=4,    # Local convolution width\n",
    "    expand=2,    # Block expansion factor\n",
    ").to(\"cuda\")\n",
    "\n",
    "y = model(x)\n",
    "assert y.shape == x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lonian/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.3\n"
     ]
    }
   ],
   "source": [
    "import mamba_ssm\n",
    "print(mamba_ssm.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lonian/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from mamba_ssm.models.config_mamba import MambaConfig\n",
    "model_config = MambaConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MambaConfig(d_model=2560, d_intermediate=0, n_layer=64, vocab_size=50277, ssm_cfg={}, attn_layer_idx=[], attn_cfg={}, rms_norm=True, residual_in_fp32=True, fused_add_norm=True, pad_vocab_size_multiple=8, tie_embeddings=True)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7830543518066406\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "output = loss(input, target)\n",
    "print(output.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lonian/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from models import MusicMambaLMHeadModel\n",
    "from mamba_ssm.models.config_mamba import MambaConfig\n",
    "import torch\n",
    "# model config setup\n",
    "model_config = MambaConfig()\n",
    "model_config.n_layer = 48\n",
    "model_config.attn_layer_idx = [11, 23, 35, 47]\n",
    "model_config.attn_cfg = {'num_heads': 8}\n",
    "model_config.vocab_size = 2048\n",
    "model_config.d_model = 1024\n",
    "model_config.ssm_cfg = {\"layer\": \"Mamba2\", \"d_state\":256}\n",
    "\n",
    "music_model = MusicMambaLMHeadModel(model_config)\n",
    "music_model = music_model.to('cuda:2')\n",
    "x = torch.randint(0, 2048, (4, 4, 1500)).to('cuda:2')\n",
    "torch.cuda.set_device(x.device.index)\n",
    "y = music_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaConfig(d_model=1024, d_intermediate=0, n_layer=48, vocab_size=40000, ssm_cfg={'layer': 'Mamba2', 'd_state': 256}, attn_layer_idx=[11, 23, 35, 47], attn_cfg={'num_heads': 8}, rms_norm=True, residual_in_fp32=True, fused_add_norm=True, pad_vocab_size_multiple=8, tie_embeddings=True)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmmodel = MambaLMHeadModel(a)\n",
    "# print(lmmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaLMHeadModel(\n",
       "  (backbone): MixerModel(\n",
       "    (embedding): Embedding(40000, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-10): 11 x Block(\n",
       "        (norm): RMSNorm()\n",
       "        (mixer): Mamba2(\n",
       "          (in_proj): Linear(in_features=1024, out_features=4640, bias=False)\n",
       "          (conv1d): Conv1d(2560, 2560, kernel_size=(4,), stride=(1,), padding=(3,), groups=2560)\n",
       "          (act): SiLU()\n",
       "          (norm): RMSNorm()\n",
       "          (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm): RMSNorm()\n",
       "        (mixer): MHA(\n",
       "          (in_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (12-22): 11 x Block(\n",
       "        (norm): RMSNorm()\n",
       "        (mixer): Mamba2(\n",
       "          (in_proj): Linear(in_features=1024, out_features=4640, bias=False)\n",
       "          (conv1d): Conv1d(2560, 2560, kernel_size=(4,), stride=(1,), padding=(3,), groups=2560)\n",
       "          (act): SiLU()\n",
       "          (norm): RMSNorm()\n",
       "          (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (23): Block(\n",
       "        (norm): RMSNorm()\n",
       "        (mixer): MHA(\n",
       "          (in_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (24-34): 11 x Block(\n",
       "        (norm): RMSNorm()\n",
       "        (mixer): Mamba2(\n",
       "          (in_proj): Linear(in_features=1024, out_features=4640, bias=False)\n",
       "          (conv1d): Conv1d(2560, 2560, kernel_size=(4,), stride=(1,), padding=(3,), groups=2560)\n",
       "          (act): SiLU()\n",
       "          (norm): RMSNorm()\n",
       "          (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (35): Block(\n",
       "        (norm): RMSNorm()\n",
       "        (mixer): MHA(\n",
       "          (in_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (36-46): 11 x Block(\n",
       "        (norm): RMSNorm()\n",
       "        (mixer): Mamba2(\n",
       "          (in_proj): Linear(in_features=1024, out_features=4640, bias=False)\n",
       "          (conv1d): Conv1d(2560, 2560, kernel_size=(4,), stride=(1,), padding=(3,), groups=2560)\n",
       "          (act): SiLU()\n",
       "          (norm): RMSNorm()\n",
       "          (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (47): Block(\n",
       "        (norm): RMSNorm()\n",
       "        (mixer): MHA(\n",
       "          (in_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_f): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=40000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_torch_model_params(model):\n",
    "    '''\n",
    "    :param model:\n",
    "    :return:\n",
    "    '''\n",
    "    # Find total parameters and trainable parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return {'total_params': total_params/1000000, 'total_trainable_params': total_trainable_params/1000000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_params': 359.79584, 'total_trainable_params': 359.79584}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_torch_model_params(lmmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_params': 309.342336, 'total_trainable_params': 309.342336}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_torch_model_params(lmmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mamba1\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = list(a)\n",
    "b.index(min(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_seq(tokens):\n",
    "    '''\n",
    "    INPUT:\n",
    "    tokens: a encodec compressed token with 4 residual layers\n",
    "    1 5 9... (course layer)\n",
    "    2 6 10...\n",
    "    3 7 11...\n",
    "    4 8 12... (fine layer)\n",
    "    \n",
    "    OUTPUT:\n",
    "    a: a flatten seq\n",
    "    1 2 3 4 5 6 7 8 9 10 11 12...\n",
    "    '''\n",
    "    K, L = tokens.shape\n",
    "    a = np.zeros((K*L))\n",
    "    for i in range(K*L):\n",
    "        a[i] = tokens[i%4, i//4]\n",
    "    return a\n",
    "\n",
    "def seq_to_token(seq):\n",
    "    '''\n",
    "    INPUT:\n",
    "    a: a flatten seq\n",
    "    1 2 3 4 5 6 7 8 9 10 11 12...\n",
    "    \n",
    "    OUTPUT:\n",
    "    tokens: a encodec compressed token with 4 residual layers\n",
    "    1 5 9... (course layer)\n",
    "    2 6 10...\n",
    "    3 7 11...\n",
    "    4 8 12... (fine layer)\n",
    "    '''\n",
    "    L = seq.shape[0]\n",
    "    print(L)\n",
    "    a = np.zeros((4, L//4))\n",
    "    idx = 0\n",
    "    for i in range(L//4):\n",
    "        for j in range(4):\n",
    "            a[j][i] = seq[idx]\n",
    "            idx+=1\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = np.load('/mnt/gestalt/home/lonian/datasets/mamba_test_token/2.npy', allow_pickle=True)\n",
    "prompt_seq = token_to_seq(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 609,  609],\n",
       "       [ 911, 2044],\n",
       "       [2004, 1895],\n",
       "       [1982, 1951]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 609.,  911., 2004., 1982.,  609., 2044., 1895., 1951.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_seq[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_seq.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n"
     ]
    }
   ],
   "source": [
    "L = prompt_seq.shape[0]\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n"
     ]
    }
   ],
   "source": [
    "b = seq_to_token(prompt_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 609.,  609.,  609., ...,  609.,  289.,  289.],\n",
       "       [ 911., 2044., 1935., ..., 2044., 1122., 2044.],\n",
       "       [2004., 1895., 2009., ..., 2019., 1947., 1895.],\n",
       "       [1982., 1951.,  268., ...,  394.,  346., 1761.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "aa = torch.Tensor(prompt_seq[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((aa, aa)).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'cat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m aa \u001b[38;5;241m=\u001b[39m prompt_seq[:\u001b[38;5;241m8\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m aa\u001b[38;5;241m.\u001b[39mcat(aa)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'cat'"
     ]
    }
   ],
   "source": [
    "aa = prompt_seq[:8]\n",
    "aa.cat(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.concatenate((aa, aa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 609.,  911., 2004., 1982.,  609., 2044., 1895., 1951.,  609.,\n",
       "        911., 2004., 1982.,  609., 2044., 1895., 1951.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lonian/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading readme: 100%|██████████| 10.5k/10.5k [00:00<00:00, 24.6MB/s]\n",
      "Downloading data: 100%|██████████| 733k/733k [00:02<00:00, 350kB/s]\n",
      "Downloading data: 100%|██████████| 6.36M/6.36M [00:00<00:00, 12.0MB/s]\n",
      "Downloading data: 100%|██████████| 657k/657k [00:01<00:00, 349kB/s]\n",
      "Generating test split: 100%|██████████| 4358/4358 [00:00<00:00, 417342.73 examples/s]\n",
      "Generating train split: 100%|██████████| 36718/36718 [00:00<00:00, 748647.17 examples/s]\n",
      "Generating validation split: 100%|██████████| 3760/3760 [00:00<00:00, 578949.45 examples/s]\n",
      "/home/lonian/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 36718/36718 [00:03<00:00, 9681.20 examples/s] \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "# from jamba.model import Jamba\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=100,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    # Create targets by shifting input_ids one token to the left\n",
    "    labels = torch.roll(input_ids, -1, dims=-1)\n",
    "    return input_ids.squeeze(), labels.squeeze()\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    tokenized_datasets,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100]) torch.Size([32, 100])\n",
      "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "for x, y in dataloader:\n",
    "    print(x.shape, y.shape)\n",
    "    print(x[:, 99]==y[:, 98])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.load('/mnt/gestalt/home/lonian/datasets/MusicBench/FMACaps_eval_set/tokendata/000166_1.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 500)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layers': 24,\n",
       " 'vocab_size': 2048,\n",
       " 'd_model': 1024,\n",
       " 'drop_p': 0.2,\n",
       " 'd_state': 512}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = meta['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lonian/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from simba import Mmamba\n",
    "device = 'cuda:0'\n",
    "model_config = {\n",
    "        'layers':40,\n",
    "        'vocab_size':2048,\n",
    "        'd_model':1024,\n",
    "        'drop_p':0.2,\n",
    "        'd_state':128,\n",
    "        }\n",
    "\n",
    "music_model = Mmamba(\n",
    "        layers = model_config['layers'], \n",
    "        vocab_size = model_config['vocab_size'], \n",
    "        d_model = model_config['d_model'], \n",
    "        drop_p = model_config['drop_p'], \n",
    "        d_state = model_config['d_state'])\n",
    "\n",
    "music_model = music_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.750249842531533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Mmamba(\n",
       "  (embedding): ModuleList(\n",
       "    (0-3): 4 x ScaledEmbedding(2048, 1024)\n",
       "  )\n",
       "  (backbone): ModuleList(\n",
       "    (0-39): 40 x MFBlock(\n",
       "      (mamba): MambaLayer(\n",
       "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path): DropPath(drop_prob=0.200)\n",
       "        (mamba): Mamba2(\n",
       "          (in_proj): Linear(in_features=1024, out_features=4384, bias=False)\n",
       "          (conv1d): Conv1d(2304, 2304, kernel_size=(4,), stride=(1,), padding=(3,), groups=2304)\n",
       "          (act): SiLU()\n",
       "          (norm): RMSNorm()\n",
       "          (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FFLayer(\n",
       "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path): DropPath(drop_prob=0.200)\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "          (1): SiLU()\n",
       "          (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): ModuleList(\n",
       "    (0-3): 4 x Linear(in_features=1024, out_features=2048, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model_path = '/mnt/gestalt/home/lonian/mamba/model/ckpts/v18/epoch_074.pkl'\n",
    "checkpoint = torch.load(model_path, map_location='cpu')\n",
    "print(checkpoint['loss'])\n",
    "music_model.load_state_dict(checkpoint['model'])\n",
    "music_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "def temperature_sampling(logits, temperature, topk):\n",
    "    # probs = np.exp(logits / temperature) / np.sum(np.exp(logits / temperature))\n",
    "    logits = torch.Tensor(logits)\n",
    "    probs = nn.Softmax(dim=0)(logits / temperature)\n",
    "    probs = np.array(probs)\n",
    "    if topk == 1:\n",
    "        prediction = np.argmax(probs)\n",
    "    else:\n",
    "        sorted_index = np.argsort(probs)[::-1]\n",
    "        candi_index = sorted_index[:topk]\n",
    "        candi_probs = [probs[i] for i in candi_index]\n",
    "        # normalize probs\n",
    "        candi_probs /= sum(candi_probs)\n",
    "        # choose by predicted probs\n",
    "        prediction = np.random.choice(candi_index, size=1, p=candi_probs)[0]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/gestalt/home/lonian/datasets/mtg_crop_sep_token/357686.npy\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import random\n",
    "\n",
    "prompt_id = random.choice(glob('/mnt/gestalt/home/lonian/datasets/mtg_crop_sep_token/*.npy'))\n",
    "print(prompt_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 (3, 4, 500)\r"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import random\n",
    "\n",
    "idx = 1\n",
    "for _ in range(10):\n",
    "    import numpy as np\n",
    "    prompt_id = glob('/mnt/gestalt/home/lonian/datasets/maestro_token/*.npy')\n",
    "    # prompt_id = glob('/mnt/gestalt/home/lonian/datasets/mtg_crop_sep_token/*.npy')\n",
    "    prompt = []\n",
    "    for i in range(3):\n",
    "        a = np.load(random.choice(prompt_id), allow_pickle=True)\n",
    "        prompt.append(a)\n",
    "    # print(prompt)\n",
    "    prompt = np.array(prompt)\n",
    "    prompt_seq = prompt[:, :, :100]\n",
    "\n",
    "    input_seq = torch.LongTensor(prompt_seq).to(device)\n",
    "    torch.cuda.set_device(input_seq.device.index)\n",
    "\n",
    "    B, K, L = prompt_seq.shape\n",
    "    while L <= 500:\n",
    "        print(L, prompt_seq.shape, end='\\r')\n",
    "        output_logits = music_model(torch.LongTensor(prompt_seq).to(device))\n",
    "        # output_logits = model(torch.LongTensor(np.array([prompt_seq])).to(device))\n",
    "        # print(output_logits.shape) # [B, 4, L+1, 2048]\n",
    "        _logit = output_logits[:, :, -1, :].to('cpu').detach().numpy()\n",
    "        batch_new = []\n",
    "        for b in range(B):\n",
    "            words = []\n",
    "            for i in range(4):\n",
    "                word = temperature_sampling(\n",
    "                        logits=_logit[b, i],\n",
    "                        temperature=1.2,\n",
    "                        topk=100)\n",
    "                words.append([word])\n",
    "            batch_new.append(words)\n",
    "        prompt_seq = np.concatenate((prompt_seq, batch_new), axis=2)\n",
    "        L+=1\n",
    "\n",
    "\n",
    "    for b in range(B):\n",
    "        np.save('/mnt/gestalt/home/lonian/mamba/model/v18_results_ckpt74_piano_prompt/{}.npy'.format(idx), prompt_seq[b])\n",
    "        idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 (3, 4, 500)\r"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "prompt_id = [2, 2, 2]\n",
    "prompt = []\n",
    "for i in prompt_id:\n",
    "    a = np.load('/mnt/gestalt/home/lonian/datasets/mamba_test_token/{}.npy'.format(i), allow_pickle=True)\n",
    "    prompt.append(a)\n",
    "# print(prompt)\n",
    "prompt = np.array(prompt)\n",
    "prompt_seq = prompt[:, :, :100]\n",
    "\n",
    "input_seq = torch.LongTensor(prompt_seq).to(device)\n",
    "torch.cuda.set_device(input_seq.device.index)\n",
    "\n",
    "B, K, L = prompt_seq.shape\n",
    "while L <= 500:\n",
    "    print(L, prompt_seq.shape, end='\\r')\n",
    "    output_logits = music_model(torch.LongTensor(prompt_seq).to(device))\n",
    "    # output_logits = model(torch.LongTensor(np.array([prompt_seq])).to(device))\n",
    "    # print(output_logits.shape) # [B, 4, L+1, 2048]\n",
    "    _logit = output_logits[:, :, -1, :].to('cpu').detach().numpy()\n",
    "    batch_new = []\n",
    "    for b in range(B):\n",
    "        words = []\n",
    "        for i in range(4):\n",
    "            word = temperature_sampling(\n",
    "                    logits=_logit[b, i],\n",
    "                    temperature=1,\n",
    "                    topk=250)\n",
    "            words.append([word])\n",
    "        batch_new.append(words)\n",
    "    prompt_seq = np.concatenate((prompt_seq, batch_new), axis=2)\n",
    "    L+=1\n",
    "\n",
    "\n",
    "idx = 7\n",
    "for b in range(B):\n",
    "    np.save('/mnt/gestalt/home/lonian/mamba/model/v18_results/{}.npy'.format(idx), prompt_seq[b])\n",
    "    idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lonian/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4301272118097728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MusicMambaLMHeadModel(\n",
       "  (embedding): ModuleList(\n",
       "    (0-3): 4 x ScaledEmbedding(2048, 1024)\n",
       "  )\n",
       "  (backbone): MusicMixerModel(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x Block(\n",
       "        (norm): RMSNorm()\n",
       "        (mixer): Mamba2(\n",
       "          (in_proj): Linear(in_features=1024, out_features=4384, bias=False)\n",
       "          (conv1d): Conv1d(2304, 2304, kernel_size=(4,), stride=(1,), padding=(3,), groups=2304)\n",
       "          (act): SiLU()\n",
       "          (norm): RMSNorm()\n",
       "          (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm): RMSNorm()\n",
       "        (mixer): MHA(\n",
       "          (in_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (9-16): 8 x Block(\n",
       "        (norm): RMSNorm()\n",
       "        (mixer): Mamba2(\n",
       "          (in_proj): Linear(in_features=1024, out_features=4384, bias=False)\n",
       "          (conv1d): Conv1d(2304, 2304, kernel_size=(4,), stride=(1,), padding=(3,), groups=2304)\n",
       "          (act): SiLU()\n",
       "          (norm): RMSNorm()\n",
       "          (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (17): Block(\n",
       "        (norm): RMSNorm()\n",
       "        (mixer): MHA(\n",
       "          (in_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (18-25): 8 x Block(\n",
       "        (norm): RMSNorm()\n",
       "        (mixer): Mamba2(\n",
       "          (in_proj): Linear(in_features=1024, out_features=4384, bias=False)\n",
       "          (conv1d): Conv1d(2304, 2304, kernel_size=(4,), stride=(1,), padding=(3,), groups=2304)\n",
       "          (act): SiLU()\n",
       "          (norm): RMSNorm()\n",
       "          (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (26): Block(\n",
       "        (norm): RMSNorm()\n",
       "        (mixer): MHA(\n",
       "          (in_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (27-34): 8 x Block(\n",
       "        (norm): RMSNorm()\n",
       "        (mixer): Mamba2(\n",
       "          (in_proj): Linear(in_features=1024, out_features=4384, bias=False)\n",
       "          (conv1d): Conv1d(2304, 2304, kernel_size=(4,), stride=(1,), padding=(3,), groups=2304)\n",
       "          (act): SiLU()\n",
       "          (norm): RMSNorm()\n",
       "          (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (35): Block(\n",
       "        (norm): RMSNorm()\n",
       "        (mixer): MHA(\n",
       "          (in_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_f): RMSNorm()\n",
       "  )\n",
       "  (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (lm_head): ModuleList(\n",
       "    (0-3): 4 x Linear(in_features=1024, out_features=2048, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import MusicMambaLMHeadModel\n",
    "import numpy as np\n",
    "from mamba_ssm.models.config_mamba import MambaConfig\n",
    "device = 'cuda:1'\n",
    "prompt_id = [2]\n",
    "prompt = []\n",
    "for i in prompt_id:\n",
    "    a = np.load('/mnt/gestalt/home/lonian/datasets/mamba_test_token/{}.npy'.format(i), allow_pickle=True)\n",
    "    prompt.append(a)\n",
    "# print(prompt)\n",
    "prompt = np.array(prompt)\n",
    "prompt_seq = prompt[:, :, :100]\n",
    "################################################\n",
    "# # v11 model config setup\n",
    "# \n",
    "# model_config = MambaConfig()\n",
    "# model_config.n_layer = 48\n",
    "# model_config.attn_layer_idx = [11, 23, 35, 47]\n",
    "# model_config.attn_cfg = {'num_heads': 16}\n",
    "# model_config.vocab_size = 2048\n",
    "# model_config.d_model = 1024\n",
    "# model_config.ssm_cfg = {\"layer\": \"Mamba2\", \"d_state\":1024}\n",
    "################################################\n",
    "# ################################################\n",
    "# # v12 model config setup\n",
    "# model_config = MambaConfig()\n",
    "# model_config.n_layer = 48\n",
    "# model_config.attn_layer_idx = [5, 11, 17, 23, 29, 35, 41, 47]\n",
    "# model_config.attn_cfg = {'num_heads': 16}\n",
    "# model_config.vocab_size = 2048\n",
    "# model_config.d_model = 1024\n",
    "# model_config.ssm_cfg = {\"layer\": \"Mamba2\", \"d_state\": 1024}\n",
    "# ################################################\n",
    "################################################\n",
    "# # v15 model config setup\n",
    "# model_config = MambaConfig()\n",
    "# model_config.n_layer = 48\n",
    "# model_config.attn_layer_idx = [\n",
    "#             4,\n",
    "#             12,\n",
    "#             20,\n",
    "#             28,\n",
    "#             36,\n",
    "#             44\n",
    "#         ]\n",
    "# model_config.attn_cfg = {'num_heads': 8}\n",
    "# model_config.vocab_size = 2048\n",
    "# model_config.d_model = 1024\n",
    "# model_config.ssm_cfg = {\"layer\": \"Mamba2\", \"d_state\": 256}\n",
    "################################################\n",
    "################################################\n",
    "# model config setup\n",
    "model_config = MambaConfig()\n",
    "model_config.n_layer = 36\n",
    "model_config.attn_layer_idx = [8, 17, 26, 35]\n",
    "# model_config.attn_layer_idx = [7, 15, 23, 31, 39, 47]\n",
    "# model_config.attn_layer_idx = [5, 11, 17, 23, 29, 35, 41, 47]\n",
    "model_config.attn_cfg = {'num_heads': 8}\n",
    "model_config.vocab_size = 2048\n",
    "model_config.d_model = 1024\n",
    "model_config.ssm_cfg = {\"layer\": \"Mamba2\", \"d_state\": 128}\n",
    "################################################\n",
    "import torch\n",
    "# model_path = '/mnt/gestalt/home/lonian/mamba/model/ckpts/v10/best.pkl'\n",
    "model_path = '/mnt/gestalt/home/lonian/mamba/model/ckpts/v17/epoch_030.pkl'\n",
    "checkpoint = torch.load(model_path, map_location='cpu')\n",
    "print(checkpoint['loss'])\n",
    "music_model = MusicMambaLMHeadModel(model_config)\n",
    "music_model = music_model.to(device)\n",
    "music_model.load_state_dict(checkpoint['model'])\n",
    "music_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MusicMambaLMHeadModel(\n",
       "  (embedding): ModuleList(\n",
       "    (0-3): 4 x ScaledEmbedding(2048, 1024)\n",
       "  )\n",
       "  (backbone): MusicMixerModel(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x Block(\n",
       "        (norm): RMSNorm()\n",
       "        (mixer): Mamba2(\n",
       "          (in_proj): Linear(in_features=1024, out_features=4640, bias=False)\n",
       "          (conv1d): Conv1d(2560, 2560, kernel_size=(4,), stride=(1,), padding=(3,), groups=2560)\n",
       "          (act): SiLU()\n",
       "          (norm): RMSNorm()\n",
       "          (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm): RMSNorm()\n",
       "        (mixer): MHA(\n",
       "          (in_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (5-11): 7 x Block(\n",
       "        (norm): RMSNorm()\n",
       "        (mixer): Mamba2(\n",
       "          (in_proj): Linear(in_features=1024, out_features=4640, bias=False)\n",
       "          (conv1d): Conv1d(2560, 2560, kernel_size=(4,), stride=(1,), padding=(3,), groups=2560)\n",
       "          (act): SiLU()\n",
       "          (norm): RMSNorm()\n",
       "          (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (12): Block(\n",
       "        (norm): RMSNorm()\n",
       "        (mixer): MHA(\n",
       "          (in_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (13-19): 7 x Block(\n",
       "        (norm): RMSNorm()\n",
       "        (mixer): Mamba2(\n",
       "          (in_proj): Linear(in_features=1024, out_features=4640, bias=False)\n",
       "          (conv1d): Conv1d(2560, 2560, kernel_size=(4,), stride=(1,), padding=(3,), groups=2560)\n",
       "          (act): SiLU()\n",
       "          (norm): RMSNorm()\n",
       "          (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (20): Block(\n",
       "        (norm): RMSNorm()\n",
       "        (mixer): MHA(\n",
       "          (in_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (21-27): 7 x Block(\n",
       "        (norm): RMSNorm()\n",
       "        (mixer): Mamba2(\n",
       "          (in_proj): Linear(in_features=1024, out_features=4640, bias=False)\n",
       "          (conv1d): Conv1d(2560, 2560, kernel_size=(4,), stride=(1,), padding=(3,), groups=2560)\n",
       "          (act): SiLU()\n",
       "          (norm): RMSNorm()\n",
       "          (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (28): Block(\n",
       "        (norm): RMSNorm()\n",
       "        (mixer): MHA(\n",
       "          (in_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (29-35): 7 x Block(\n",
       "        (norm): RMSNorm()\n",
       "        (mixer): Mamba2(\n",
       "          (in_proj): Linear(in_features=1024, out_features=4640, bias=False)\n",
       "          (conv1d): Conv1d(2560, 2560, kernel_size=(4,), stride=(1,), padding=(3,), groups=2560)\n",
       "          (act): SiLU()\n",
       "          (norm): RMSNorm()\n",
       "          (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (36): Block(\n",
       "        (norm): RMSNorm()\n",
       "        (mixer): MHA(\n",
       "          (in_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (37-43): 7 x Block(\n",
       "        (norm): RMSNorm()\n",
       "        (mixer): Mamba2(\n",
       "          (in_proj): Linear(in_features=1024, out_features=4640, bias=False)\n",
       "          (conv1d): Conv1d(2560, 2560, kernel_size=(4,), stride=(1,), padding=(3,), groups=2560)\n",
       "          (act): SiLU()\n",
       "          (norm): RMSNorm()\n",
       "          (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (44): Block(\n",
       "        (norm): RMSNorm()\n",
       "        (mixer): MHA(\n",
       "          (in_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (45-47): 3 x Block(\n",
       "        (norm): RMSNorm()\n",
       "        (mixer): Mamba2(\n",
       "          (in_proj): Linear(in_features=1024, out_features=4640, bias=False)\n",
       "          (conv1d): Conv1d(2560, 2560, kernel_size=(4,), stride=(1,), padding=(3,), groups=2560)\n",
       "          (act): SiLU()\n",
       "          (norm): RMSNorm()\n",
       "          (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_f): RMSNorm()\n",
       "  )\n",
       "  (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (lm_head): ModuleList(\n",
       "    (0-3): 4 x Linear(in_features=1024, out_features=2048, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# music_model1 = MusicMambaLMHeadModel(model_config)\n",
    "# music_model1 = music_model1.to(device)\n",
    "# music_model1.load_state_dict(checkpoint['model'])\n",
    "# music_model1.eval()\n",
    "\n",
    "# music_model2 = MusicMambaLMHeadModel(model_config)\n",
    "# music_model2 = music_model2.to(device)\n",
    "# music_model2.load_state_dict(checkpoint['model'])\n",
    "# music_model2.eval()\n",
    "\n",
    "music_model3 = MusicMambaLMHeadModel(model_config)\n",
    "music_model3 = music_model3.to(device)\n",
    "music_model3.load_state_dict(checkpoint['model'])\n",
    "music_model3.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_id = [2]\n",
    "prompt = []\n",
    "for i in prompt_id:\n",
    "    a = np.load('/mnt/gestalt/home/lonian/datasets/mamba_test_token/{}.npy'.format(i), allow_pickle=True)\n",
    "    prompt.append(a)\n",
    "# print(prompt)\n",
    "prompt = np.array(prompt)\n",
    "prompt_seq = prompt[:, :, :100]\n",
    "\n",
    "input_seq = torch.LongTensor(prompt_seq).to(device)\n",
    "torch.cuda.set_device(input_seq.device.index)\n",
    "out = music_model.generate(input_ids=input_seq, max_length=1500, temperature=1.5, top_k=100, top_p=0.9, repetition_penalty=2)\n",
    "# out = music_model.generate(input_ids=input_seq, max_length=1500, temperature=1.5, top_k=25, top_p=0.9, repetition_penalty=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = '19'\n",
    "out = out.cpu().numpy()\n",
    "np.save('/mnt/gestalt/home/lonian/mamba/model/v17_results/ckpt26/{}.npy'.format(idx), out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m input_seq \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(prompt_seq)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mset_device(input_seq\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m---> 14\u001b[0m out \u001b[38;5;241m=\u001b[39m music_model\u001b[38;5;241m.\u001b[39mgenerate(input_ids\u001b[38;5;241m=\u001b[39minput_seq, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1500\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.2\u001b[39m, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m250\u001b[39m, top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m)\n\u001b[1;32m     15\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     16\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mnt/gestalt/home/lonian/mamba/model/v17_results/ckpt26/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(idx), out[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/mamba_ssm/utils/generation.py:412\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, input_ids, max_length, top_k, top_p, min_p, temperature, return_dict_in_generate, output_scores, **kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    399\u001b[0m     input_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;66;03m#     input_ids, self, max_length, top_k=top_k, top_p=top_p, min_p = min_p, temperature=temperature, **kwargs\u001b[39;00m\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m--> 412\u001b[0m     output \u001b[38;5;241m=\u001b[39m decode_music(\n\u001b[1;32m    413\u001b[0m         input_ids, \u001b[38;5;28mself\u001b[39m, max_length, top_k\u001b[38;5;241m=\u001b[39mtop_k, top_p\u001b[38;5;241m=\u001b[39mtop_p, min_p \u001b[38;5;241m=\u001b[39m min_p, temperature\u001b[38;5;241m=\u001b[39mtemperature, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    414\u001b[0m     )\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_scores:\n\u001b[1;32m    416\u001b[0m         output\u001b[38;5;241m.\u001b[39mscores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/mamba_ssm/utils/generation.py:359\u001b[0m, in \u001b[0;36mdecode_music\u001b[0;34m(input_ids, model, max_length, top_k, top_p, min_p, temperature, repetition_penalty, eos_token_id, teacher_outputs, vocab_size, cg, enable_timing, streamer)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# sequences_cat = (B, K, S)\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_stop(sequences[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], inference_params):\n\u001b[0;32m--> 359\u001b[0m     scores\u001b[38;5;241m.\u001b[39mappend(get_logits(sequences[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], inference_params))\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;66;03m# print(scores[-1].shape, type(scores[-1]), scores)\u001b[39;00m\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;66;03m# logit = scores[-1] = (B, K, 1, 2048)\u001b[39;00m\n\u001b[1;32m    362\u001b[0m     inference_params\u001b[38;5;241m.\u001b[39mseqlen_offset \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sequences[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/mamba_ssm/utils/generation.py:316\u001b[0m, in \u001b[0;36mdecode_music.<locals>.get_logits\u001b[0;34m(input_ids, inference_params)\u001b[0m\n\u001b[1;32m    314\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cg \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m decoding:\n\u001b[0;32m--> 316\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m    317\u001b[0m         input_ids,\n\u001b[1;32m    318\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    319\u001b[0m         inference_params\u001b[38;5;241m=\u001b[39minference_params,\n\u001b[1;32m    320\u001b[0m         num_last_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    321\u001b[0m     )\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39msqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_decoding_cache\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m    324\u001b[0m         input_ids, position_ids, inference_params\u001b[38;5;241m.\u001b[39mseqlen_offset\n\u001b[1;32m    325\u001b[0m     )\u001b[38;5;241m.\u001b[39msqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/gestalt/home/lonian/mamba/model/models.py:495\u001b[0m, in \u001b[0;36mMusicMambaLMHeadModel.forward\u001b[0;34m(self, input_ids, position_ids, inference_params, num_last_tokens, **mixer_kwargs)\u001b[0m\n\u001b[1;32m    492\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding[k](input_ids[:, k]) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(K)])\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# print(input_ids)\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;66;03m# input: (B, K, S) => output: (B, S)\u001b[39;00m\n\u001b[0;32m--> 495\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone(input_ids, inference_params\u001b[38;5;241m=\u001b[39minference_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmixer_kwargs)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_last_tokens \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    497\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states[:, \u001b[38;5;241m-\u001b[39mnum_last_tokens:]\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/gestalt/home/lonian/mamba/model/models.py:401\u001b[0m, in \u001b[0;36mMusicMixerModel.forward\u001b[0;34m(self, input_ids, inference_params, **mixer_kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 401\u001b[0m     hidden_states, residual \u001b[38;5;241m=\u001b[39m layer(\n\u001b[1;32m    402\u001b[0m         hidden_states, residual, inference_params\u001b[38;5;241m=\u001b[39minference_params\n\u001b[1;32m    403\u001b[0m     )\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfused_add_norm:\n\u001b[1;32m    405\u001b[0m     residual \u001b[38;5;241m=\u001b[39m (hidden_states \u001b[38;5;241m+\u001b[39m residual) \u001b[38;5;28;01mif\u001b[39;00m residual \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/mamba_ssm/modules/block.py:67\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, hidden_states, residual, inference_params, **mixer_kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     hidden_states, residual \u001b[38;5;241m=\u001b[39m layer_norm_fn(\n\u001b[1;32m     58\u001b[0m         hidden_states,\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m         is_rms_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm, RMSNorm)\n\u001b[1;32m     66\u001b[0m     )\n\u001b[0;32m---> 67\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmixer(hidden_states, inference_params\u001b[38;5;241m=\u001b[39minference_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmixer_kwargs)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfused_add_norm:\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/mamba_ssm/modules/mamba2.py:167\u001b[0m, in \u001b[0;36mMamba2.forward\u001b[0;34m(self, u, seqlen, seq_idx, inference_params)\u001b[0m\n\u001b[1;32m    164\u001b[0m     conv_state, ssm_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_states_from_cache(inference_params, batch)\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inference_params\u001b[38;5;241m.\u001b[39mseqlen_offset \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;66;03m# The states are updated inplace\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m         out, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep(u, conv_state, ssm_state)\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m    170\u001b[0m zxbcdt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj(u)  \u001b[38;5;66;03m# (B, L, d_in_proj) or (B * L, d_in_proj)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/mamba_ssm/modules/mamba2.py:312\u001b[0m, in \u001b[0;36mMamba2.step\u001b[0;34m(self, hidden_states, conv_state, ssm_state)\u001b[0m\n\u001b[1;32m    307\u001b[0m         z \u001b[38;5;241m=\u001b[39m rearrange(z, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb (h p) -> b h p\u001b[39m\u001b[38;5;124m\"\u001b[39m, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaddim)\n\u001b[1;32m    308\u001b[0m     y \u001b[38;5;241m=\u001b[39m selective_state_update(\n\u001b[1;32m    309\u001b[0m         ssm_state, x_reshaped, dt, A, B, C, D, z\u001b[38;5;241m=\u001b[39mz \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrmsnorm \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m         dt_bias\u001b[38;5;241m=\u001b[39mdt_bias, dt_softplus\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     )\n\u001b[0;32m--> 312\u001b[0m     y \u001b[38;5;241m=\u001b[39m rearrange(y, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb h p -> b (h p)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrmsnorm:\n\u001b[1;32m    314\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(y, z)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/einops/einops.py:591\u001b[0m, in \u001b[0;36mrearrange\u001b[0;34m(tensor, pattern, **axes_lengths)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrearrange\u001b[39m(tensor: Union[Tensor, List[Tensor]], pattern: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39maxes_lengths) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    537\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;124;03m    einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors.\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m    This operation includes functionality of transpose (axes permutation), reshape (view), squeeze, unsqueeze,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m \n\u001b[1;32m    590\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reduce(tensor, pattern, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrearrange\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39maxes_lengths)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/einops/einops.py:523\u001b[0m, in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    521\u001b[0m     shape \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mshape(tensor)\n\u001b[1;32m    522\u001b[0m     recipe \u001b[38;5;241m=\u001b[39m _prepare_transformation_recipe(pattern, reduction, axes_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(axes_lengths), ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(shape))\n\u001b[0;32m--> 523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _apply_recipe(\n\u001b[1;32m    524\u001b[0m         backend, recipe, cast(Tensor, tensor), reduction_type\u001b[38;5;241m=\u001b[39mreduction, axes_lengths\u001b[38;5;241m=\u001b[39mhashable_axes_lengths\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EinopsError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    527\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error while processing \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m-reduction pattern \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(reduction, pattern)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/einops/einops.py:234\u001b[0m, in \u001b[0;36m_apply_recipe\u001b[0;34m(backend, recipe, tensor, reduction_type, axes_lengths)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply_recipe\u001b[39m(\n\u001b[1;32m    230\u001b[0m     backend, recipe: TransformRecipe, tensor: Tensor, reduction_type: Reduction, axes_lengths: HashableAxesLengths\n\u001b[1;32m    231\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;66;03m# this method implements actual work for all backends for 3 operations\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m         init_shapes, axes_reordering, reduced_axes, added_axes, final_shapes, n_axes_w_added \u001b[38;5;241m=\u001b[39m _reconstruct_from_shape(\n\u001b[1;32m    235\u001b[0m             recipe, backend\u001b[38;5;241m.\u001b[39mshape(tensor), axes_lengths\n\u001b[1;32m    236\u001b[0m         )\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;66;03m# shape or one of passed axes lengths is not hashable (i.e. they are symbols)\u001b[39;00m\n\u001b[1;32m    239\u001b[0m         _result \u001b[38;5;241m=\u001b[39m _reconstruct_from_shape_uncached(recipe, backend\u001b[38;5;241m.\u001b[39mshape(tensor), axes_lengths)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# idx = '1'\n",
    "for i in range(10):\n",
    "    idx = i+10\n",
    "    prompt_id = [idx+1]\n",
    "    prompt = []\n",
    "    for i in prompt_id:\n",
    "        a = np.load('/mnt/gestalt/home/lonian/datasets/mamba_test_token/{}.npy'.format(i), allow_pickle=True)\n",
    "        prompt.append(a)\n",
    "    # print(prompt)\n",
    "    prompt = np.array(prompt)\n",
    "    prompt_seq = prompt[:, :, :100]\n",
    "    input_seq = torch.LongTensor(prompt_seq).to(device)\n",
    "    torch.cuda.set_device(input_seq.device.index)\n",
    "    out = music_model.generate(input_ids=input_seq, max_length=1500, temperature=1.2, top_k=250, top_p=0.95)\n",
    "    out = out.cpu().numpy()\n",
    "    np.save('/mnt/gestalt/home/lonian/mamba/model/v17_results/ckpt26/{}.npy'.format(idx), out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1499 (1, 4, 1499)\r"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "def temperature_sampling(logits, temperature, topk):\n",
    "    # probs = np.exp(logits / temperature) / np.sum(np.exp(logits / temperature))\n",
    "    logits = torch.Tensor(logits)\n",
    "    probs = nn.Softmax(dim=0)(logits / temperature)\n",
    "    probs = np.array(probs)\n",
    "    if topk == 1:\n",
    "        prediction = np.argmax(probs)\n",
    "    else:\n",
    "        sorted_index = np.argsort(probs)[::-1]\n",
    "        candi_index = sorted_index[:topk]\n",
    "        candi_probs = [probs[i] for i in candi_index]\n",
    "        # normalize probs\n",
    "        candi_probs /= sum(candi_probs)\n",
    "        # choose by predicted probs\n",
    "        prediction = np.random.choice(candi_index, size=1, p=candi_probs)[0]\n",
    "    return prediction\n",
    "L = 100\n",
    "while L < 1500:\n",
    "    # input_seq = prompt_seq[:, -1499:]\n",
    "    print(L, prompt_seq.shape, end='\\r')\n",
    "    # print(input_seq.shape)\n",
    "    # if L == prompt_length:\n",
    "    #     output_logits = model(torch.LongTensor(prompt_seq).to(device))\n",
    "    #     print(prompt_seq.shape)\n",
    "    # else:\n",
    "    #     output_logits = model(torch.LongTensor(prompt_seq[:, :, -1:]).to(device))\n",
    "    output_logits = music_model(torch.LongTensor(prompt_seq).to(device))\n",
    "    # output_logits = model(torch.LongTensor(np.array([prompt_seq])).to(device))\n",
    "    # print(output_logits.shape) # [B, 4, L+1, 2048]\n",
    "    _logit = output_logits.logits[:, :, -1, :].to('cpu').detach().numpy()\n",
    "    batch_new = []\n",
    "    for b in range(1):\n",
    "        words = []\n",
    "        for i in range(4):\n",
    "            word = temperature_sampling(\n",
    "                    logits=_logit[b, i],\n",
    "                    temperature=1.2,\n",
    "                    topk=200)\n",
    "            words.append([word])\n",
    "        batch_new.append(words)\n",
    "    prompt_seq = np.concatenate((prompt_seq, batch_new), axis=2)\n",
    "    # L = prompt_seq.shape[1]\n",
    "    L+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(1):\n",
    "    np.save('/mnt/gestalt/home/lonian/mamba/model/results/new.npy', prompt_seq[b])\n",
    "    # idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class InferenceParams:\n",
    "    \"\"\"Inference parameters that are passed to the main model in order\n",
    "    to efficienly calculate and store the context during inference.\"\"\"\n",
    "\n",
    "    max_seqlen: int\n",
    "    max_batch_size: int\n",
    "    seqlen_offset: int = 0\n",
    "    batch_size_offset: int = 0\n",
    "    key_value_memory_dict: dict = field(default_factory=dict)\n",
    "    lengths_per_sample: Optional[Tensor] = None\n",
    "\n",
    "    def reset(self, max_seqlen, max_batch_size):\n",
    "        self.max_seqlen = max_seqlen\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.seqlen_offset = 0\n",
    "        if self.lengths_per_sample is not None:\n",
    "            self.lengths_per_sample.zero_()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "path = glob('/mnt/gestalt/home/lonian/datasets/maestro_token/*.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23282"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_list = [{}, {}, {}, {}]\n",
    "\n",
    "for p in path:\n",
    "    token = np.load(p, allow_pickle=True)\n",
    "    # print(token.shape)\n",
    "    # print(len(np.unique(token[0])))\n",
    "    for k in range(4):\n",
    "        for i in np.unique(token[k]):\n",
    "            if i not in list(dict_list[k].keys()):\n",
    "                dict_list[k][i] = np.count_nonzero(token[k] == i)\n",
    "            else:\n",
    "                num = np.count_nonzero(token[k] == i)\n",
    "                # print(num)\n",
    "                dict_list[k][i] += np.count_nonzero(token[k] == i)\n",
    "            # print()\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192\n",
      "1565\n",
      "1525\n",
      "1641\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(len(list(dict_list[i].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8: 1432316,\n",
       " 35: 134240,\n",
       " 40: 5630,\n",
       " 42: 119987,\n",
       " 50: 89401,\n",
       " 57: 78752,\n",
       " 66: 76373,\n",
       " 80: 8852,\n",
       " 83: 1987963,\n",
       " 108: 56686,\n",
       " 121: 54355,\n",
       " 160: 72099,\n",
       " 162: 121772,\n",
       " 166: 68686,\n",
       " 184: 47748,\n",
       " 202: 34320,\n",
       " 203: 99597,\n",
       " 233: 123387,\n",
       " 237: 48852,\n",
       " 247: 265352,\n",
       " 248: 196795,\n",
       " 263: 114166,\n",
       " 271: 17012,\n",
       " 277: 18952,\n",
       " 289: 3347843,\n",
       " 290: 224049,\n",
       " 314: 359728,\n",
       " 315: 33562,\n",
       " 328: 62928,\n",
       " 345: 76148,\n",
       " 367: 176366,\n",
       " 376: 37,\n",
       " 391: 46385,\n",
       " 404: 107557,\n",
       " 407: 270347,\n",
       " 415: 3743,\n",
       " 441: 34735,\n",
       " 443: 372812,\n",
       " 452: 12325,\n",
       " 465: 265167,\n",
       " 470: 302448,\n",
       " 472: 1509,\n",
       " 494: 65969,\n",
       " 506: 292538,\n",
       " 508: 28151,\n",
       " 518: 99194,\n",
       " 528: 34533,\n",
       " 537: 12823,\n",
       " 563: 47787,\n",
       " 609: 848834,\n",
       " 618: 9754,\n",
       " 620: 48447,\n",
       " 624: 13886,\n",
       " 627: 16706,\n",
       " 634: 85547,\n",
       " 643: 43115,\n",
       " 648: 282499,\n",
       " 654: 204246,\n",
       " 658: 77914,\n",
       " 710: 18887,\n",
       " 712: 172958,\n",
       " 715: 14620,\n",
       " 720: 127282,\n",
       " 731: 39553,\n",
       " 734: 53811,\n",
       " 754: 19887,\n",
       " 773: 175892,\n",
       " 778: 60910,\n",
       " 793: 273359,\n",
       " 801: 189565,\n",
       " 832: 74374,\n",
       " 840: 46344,\n",
       " 851: 57277,\n",
       " 852: 29904,\n",
       " 879: 22408,\n",
       " 888: 105487,\n",
       " 905: 2007,\n",
       " 913: 17753,\n",
       " 914: 56628,\n",
       " 915: 71441,\n",
       " 917: 278307,\n",
       " 919: 29137,\n",
       " 937: 125038,\n",
       " 938: 1638,\n",
       " 939: 43859,\n",
       " 942: 93987,\n",
       " 945: 48272,\n",
       " 954: 41997,\n",
       " 965: 391305,\n",
       " 976: 51196,\n",
       " 982: 223290,\n",
       " 985: 252812,\n",
       " 986: 59583,\n",
       " 999: 441538,\n",
       " 1001: 30258,\n",
       " 1004: 73560,\n",
       " 1027: 38525,\n",
       " 1038: 29619,\n",
       " 1044: 44301,\n",
       " 1074: 79382,\n",
       " 1075: 199520,\n",
       " 1078: 28301,\n",
       " 1095: 118351,\n",
       " 1111: 28503,\n",
       " 1130: 105941,\n",
       " 1159: 648,\n",
       " 1173: 284135,\n",
       " 1176: 71519,\n",
       " 1183: 97388,\n",
       " 1188: 137658,\n",
       " 1193: 530850,\n",
       " 1194: 24081,\n",
       " 1204: 75056,\n",
       " 1238: 2218,\n",
       " 1254: 39993,\n",
       " 1265: 1632,\n",
       " 1268: 99500,\n",
       " 1269: 14084,\n",
       " 1271: 317765,\n",
       " 1277: 39437,\n",
       " 1295: 54735,\n",
       " 1299: 12919,\n",
       " 1310: 20849,\n",
       " 1311: 10749,\n",
       " 1314: 45622,\n",
       " 1325: 442091,\n",
       " 1335: 36506,\n",
       " 1342: 4881,\n",
       " 1348: 406958,\n",
       " 1358: 11553,\n",
       " 1364: 9150,\n",
       " 1368: 7185,\n",
       " 1388: 248403,\n",
       " 1389: 9154,\n",
       " 1415: 5873,\n",
       " 1417: 130855,\n",
       " 1418: 29143,\n",
       " 1436: 57185,\n",
       " 1437: 147847,\n",
       " 1452: 44067,\n",
       " 1453: 385466,\n",
       " 1454: 221871,\n",
       " 1456: 4692,\n",
       " 1470: 67000,\n",
       " 1479: 105614,\n",
       " 1497: 4543,\n",
       " 1500: 38648,\n",
       " 1529: 44403,\n",
       " 1542: 1702,\n",
       " 1544: 24830,\n",
       " 1548: 326259,\n",
       " 1550: 86969,\n",
       " 1554: 84687,\n",
       " 1561: 59406,\n",
       " 1572: 44629,\n",
       " 1577: 28047,\n",
       " 1589: 1934,\n",
       " 1590: 29147,\n",
       " 1598: 25299,\n",
       " 1614: 18129,\n",
       " 1618: 41363,\n",
       " 1629: 60887,\n",
       " 1634: 236491,\n",
       " 1638: 13933,\n",
       " 1652: 52419,\n",
       " 1658: 49377,\n",
       " 1662: 30737,\n",
       " 1667: 404351,\n",
       " 1670: 70902,\n",
       " 1673: 8314,\n",
       " 1706: 316536,\n",
       " 1711: 321645,\n",
       " 1713: 51286,\n",
       " 1716: 62990,\n",
       " 1720: 218565,\n",
       " 1735: 140176,\n",
       " 1756: 275505,\n",
       " 1757: 317056,\n",
       " 1771: 7309,\n",
       " 1785: 71674,\n",
       " 1788: 176660,\n",
       " 1795: 48356,\n",
       " 1797: 210002,\n",
       " 1821: 88043,\n",
       " 1823: 40137,\n",
       " 1825: 64939,\n",
       " 1842: 110108,\n",
       " 1850: 11024,\n",
       " 1871: 102884,\n",
       " 1875: 14565,\n",
       " 1895: 32252,\n",
       " 1908: 272548,\n",
       " 1923: 339034,\n",
       " 1950: 6500,\n",
       " 1958: 213760,\n",
       " 1967: 4438,\n",
       " 1968: 167475,\n",
       " 1974: 131815,\n",
       " 1977: 17814,\n",
       " 1978: 426720,\n",
       " 1980: 53844,\n",
       " 1984: 27391,\n",
       " 2001: 43994,\n",
       " 2036: 28955,\n",
       " 2042: 21721,\n",
       " 4: 3074,\n",
       " 16: 4941,\n",
       " 90: 27153,\n",
       " 138: 36946,\n",
       " 178: 29145,\n",
       " 234: 47158,\n",
       " 251: 5928,\n",
       " 253: 3050,\n",
       " 278: 8646,\n",
       " 286: 29034,\n",
       " 287: 2630,\n",
       " 300: 3469,\n",
       " 325: 22094,\n",
       " 336: 1821,\n",
       " 339: 218,\n",
       " 410: 176080,\n",
       " 418: 19452,\n",
       " 437: 28389,\n",
       " 446: 65851,\n",
       " 457: 2781,\n",
       " 462: 1730,\n",
       " 483: 3199,\n",
       " 484: 15418,\n",
       " 487: 12630,\n",
       " 501: 7934,\n",
       " 511: 27579,\n",
       " 519: 4913,\n",
       " 530: 252945,\n",
       " 548: 24819,\n",
       " 564: 230587,\n",
       " 575: 37321,\n",
       " 585: 9201,\n",
       " 589: 62161,\n",
       " 612: 12845,\n",
       " 615: 3905,\n",
       " 644: 85183,\n",
       " 649: 5338,\n",
       " 670: 4747,\n",
       " 677: 28985,\n",
       " 685: 18006,\n",
       " 693: 2314,\n",
       " 701: 2725,\n",
       " 705: 37571,\n",
       " 708: 32987,\n",
       " 714: 5381,\n",
       " 727: 5803,\n",
       " 747: 74150,\n",
       " 764: 5771,\n",
       " 770: 23860,\n",
       " 776: 34561,\n",
       " 811: 3060,\n",
       " 813: 6208,\n",
       " 822: 9168,\n",
       " 825: 18639,\n",
       " 869: 8777,\n",
       " 884: 4501,\n",
       " 890: 11143,\n",
       " 893: 8182,\n",
       " 897: 1829,\n",
       " 902: 8088,\n",
       " 933: 3624,\n",
       " 936: 33976,\n",
       " 951: 8406,\n",
       " 975: 14340,\n",
       " 981: 56328,\n",
       " 987: 7253,\n",
       " 1013: 29262,\n",
       " 1016: 17903,\n",
       " 1019: 4071,\n",
       " 1100: 3295,\n",
       " 1103: 16749,\n",
       " 1104: 10142,\n",
       " 1106: 17696,\n",
       " 1124: 2232,\n",
       " 1131: 101229,\n",
       " 1134: 27504,\n",
       " 1137: 17711,\n",
       " 1146: 34545,\n",
       " 1155: 28222,\n",
       " 1170: 3806,\n",
       " 1174: 6924,\n",
       " 1220: 22808,\n",
       " 1222: 38849,\n",
       " 1224: 27134,\n",
       " 1225: 50187,\n",
       " 1231: 38211,\n",
       " 1250: 6451,\n",
       " 1264: 97069,\n",
       " 1280: 1363,\n",
       " 1286: 24908,\n",
       " 1301: 12073,\n",
       " 1319: 40143,\n",
       " 1328: 9395,\n",
       " 1338: 8865,\n",
       " 1347: 8243,\n",
       " 1350: 59902,\n",
       " 1356: 6093,\n",
       " 1363: 3616,\n",
       " 1373: 65426,\n",
       " 1390: 15048,\n",
       " 1409: 10502,\n",
       " 1416: 14329,\n",
       " 1419: 41268,\n",
       " 1426: 54426,\n",
       " 1433: 5301,\n",
       " 1445: 100343,\n",
       " 1458: 9418,\n",
       " 1462: 3695,\n",
       " 1508: 14569,\n",
       " 1513: 13706,\n",
       " 1536: 14754,\n",
       " 1538: 10117,\n",
       " 1555: 48365,\n",
       " 1594: 38133,\n",
       " 1600: 23151,\n",
       " 1603: 4793,\n",
       " 1620: 9592,\n",
       " 1688: 107,\n",
       " 1693: 93832,\n",
       " 1700: 121645,\n",
       " 1702: 117594,\n",
       " 1721: 34445,\n",
       " 1748: 10265,\n",
       " 1764: 583,\n",
       " 1778: 30264,\n",
       " 1791: 4947,\n",
       " 1807: 5093,\n",
       " 1827: 85731,\n",
       " 1831: 2400,\n",
       " 1835: 9810,\n",
       " 1837: 12316,\n",
       " 1846: 3929,\n",
       " 1847: 2089,\n",
       " 1900: 143524,\n",
       " 1907: 6521,\n",
       " 1991: 65445,\n",
       " 2034: 2519,\n",
       " 2: 254,\n",
       " 17: 36219,\n",
       " 28: 2362,\n",
       " 31: 98,\n",
       " 73: 1011,\n",
       " 118: 35863,\n",
       " 126: 162956,\n",
       " 141: 2758,\n",
       " 153: 6034,\n",
       " 187: 1227,\n",
       " 215: 3997,\n",
       " 218: 8889,\n",
       " 241: 9492,\n",
       " 264: 2629,\n",
       " 297: 6213,\n",
       " 341: 18789,\n",
       " 393: 608,\n",
       " 432: 1806,\n",
       " 448: 15379,\n",
       " 453: 7822,\n",
       " 466: 4486,\n",
       " 469: 8581,\n",
       " 503: 2212,\n",
       " 523: 8091,\n",
       " 550: 3604,\n",
       " 560: 20667,\n",
       " 562: 7862,\n",
       " 569: 1684,\n",
       " 580: 1917,\n",
       " 637: 5803,\n",
       " 639: 6733,\n",
       " 642: 80665,\n",
       " 741: 8817,\n",
       " 762: 1193,\n",
       " 789: 5253,\n",
       " 824: 4240,\n",
       " 826: 28268,\n",
       " 870: 1119,\n",
       " 900: 1603,\n",
       " 903: 20244,\n",
       " 932: 6531,\n",
       " 950: 2294,\n",
       " 958: 257,\n",
       " 960: 19335,\n",
       " 1005: 3238,\n",
       " 1010: 18253,\n",
       " 1051: 2119,\n",
       " 1113: 9247,\n",
       " 1114: 1721,\n",
       " 1152: 1739,\n",
       " 1175: 25392,\n",
       " 1198: 3980,\n",
       " 1203: 8014,\n",
       " 1217: 11332,\n",
       " 1246: 41403,\n",
       " 1257: 1342,\n",
       " 1303: 1329,\n",
       " 1316: 451,\n",
       " 1339: 113,\n",
       " 1361: 38320,\n",
       " 1365: 580,\n",
       " 1385: 2788,\n",
       " 1391: 1467,\n",
       " 1410: 423,\n",
       " 1444: 44207,\n",
       " 1469: 2585,\n",
       " 1472: 4170,\n",
       " 1478: 56160,\n",
       " 1492: 12299,\n",
       " 1511: 15454,\n",
       " 1522: 1964,\n",
       " 1532: 210,\n",
       " 1541: 89076,\n",
       " 1557: 1565,\n",
       " 1595: 18167,\n",
       " 1627: 8039,\n",
       " 1642: 9497,\n",
       " 1654: 46237,\n",
       " 1697: 939,\n",
       " 1710: 4002,\n",
       " 1743: 10898,\n",
       " 1744: 23395,\n",
       " 1762: 37671,\n",
       " 1828: 37929,\n",
       " 1854: 6842,\n",
       " 1855: 3334,\n",
       " 1869: 1573,\n",
       " 1876: 1872,\n",
       " 1905: 11369,\n",
       " 1915: 342,\n",
       " 1924: 8377,\n",
       " 1933: 488,\n",
       " 1947: 23371,\n",
       " 1959: 813,\n",
       " 1976: 2351,\n",
       " 1990: 1050,\n",
       " 1995: 1191,\n",
       " 1998: 6554,\n",
       " 2010: 18844,\n",
       " 2045: 3551,\n",
       " 2046: 3212,\n",
       " 77: 107802,\n",
       " 78: 2067,\n",
       " 103: 18102,\n",
       " 183: 22020,\n",
       " 252: 9920,\n",
       " 261: 1140,\n",
       " 311: 4380,\n",
       " 423: 625,\n",
       " 505: 6677,\n",
       " 532: 1294,\n",
       " 594: 15636,\n",
       " 676: 12145,\n",
       " 730: 25621,\n",
       " 807: 5089,\n",
       " 809: 15655,\n",
       " 821: 3650,\n",
       " 828: 98925,\n",
       " 875: 10163,\n",
       " 901: 11622,\n",
       " 952: 1690,\n",
       " 967: 13122,\n",
       " 1034: 4449,\n",
       " 1037: 1009,\n",
       " 1050: 4328,\n",
       " 1059: 7936,\n",
       " 1076: 1901,\n",
       " 1082: 17344,\n",
       " 1083: 11013,\n",
       " 1214: 1380,\n",
       " 1336: 2277,\n",
       " 1337: 464,\n",
       " 1370: 83548,\n",
       " 1429: 11710,\n",
       " 1460: 26041,\n",
       " 1507: 108028,\n",
       " 1516: 4972,\n",
       " 1521: 8521,\n",
       " 1531: 1327,\n",
       " 1608: 2879,\n",
       " 1628: 7077,\n",
       " 1632: 14774,\n",
       " 1727: 4591,\n",
       " 1752: 3724,\n",
       " 1868: 10546,\n",
       " 1898: 2192,\n",
       " 1911: 2080,\n",
       " 1962: 592,\n",
       " 2018: 4970,\n",
       " 6: 7139,\n",
       " 71: 1715,\n",
       " 366: 16561,\n",
       " 440: 19742,\n",
       " 520: 4444,\n",
       " 559: 7980,\n",
       " 617: 6495,\n",
       " 662: 5479,\n",
       " 725: 3016,\n",
       " 881: 2817,\n",
       " 918: 2141,\n",
       " 929: 4599,\n",
       " 994: 5332,\n",
       " 1186: 2515,\n",
       " 1201: 1884,\n",
       " 1213: 42956,\n",
       " 1253: 1285,\n",
       " 1284: 7740,\n",
       " 1525: 8175,\n",
       " 1556: 283,\n",
       " 1571: 7099,\n",
       " 1575: 645,\n",
       " 1616: 21261,\n",
       " 1639: 969,\n",
       " 1650: 1800,\n",
       " 1723: 13060,\n",
       " 1794: 1097,\n",
       " 1878: 3951,\n",
       " 1945: 8779,\n",
       " 2015: 7473,\n",
       " 2038: 7218,\n",
       " 922: 6730,\n",
       " 1309: 3364,\n",
       " 1918: 2205,\n",
       " 1996: 7344,\n",
       " 15: 9408,\n",
       " 285: 12447,\n",
       " 500: 2418,\n",
       " 753: 27638,\n",
       " 940: 7056,\n",
       " 1007: 250,\n",
       " 1052: 1103,\n",
       " 1115: 5565,\n",
       " 1212: 11557,\n",
       " 1343: 1346,\n",
       " 1354: 12035,\n",
       " 1394: 3219,\n",
       " 1447: 520,\n",
       " 1451: 534,\n",
       " 1506: 5169,\n",
       " 1530: 8169,\n",
       " 1568: 263,\n",
       " 1672: 366,\n",
       " 1732: 3163,\n",
       " 1930: 2719,\n",
       " 3: 10926,\n",
       " 82: 2412,\n",
       " 189: 2672,\n",
       " 338: 1776,\n",
       " 626: 2505,\n",
       " 1344: 3303,\n",
       " 1701: 1048,\n",
       " 1810: 5446,\n",
       " 1884: 175,\n",
       " 2031: 3792,\n",
       " 58: 7756,\n",
       " 167: 654,\n",
       " 436: 1923,\n",
       " 711: 1575,\n",
       " 836: 2064,\n",
       " 984: 6439,\n",
       " 1202: 504,\n",
       " 1333: 12875,\n",
       " 1351: 1965,\n",
       " 1367: 5420,\n",
       " 1402: 1561,\n",
       " 1646: 37519,\n",
       " 205: 9477,\n",
       " 1242: 9484,\n",
       " 158: 1809,\n",
       " 674: 3595,\n",
       " 749: 3744,\n",
       " 1251: 4519,\n",
       " 1318: 9773,\n",
       " 1381: 3743,\n",
       " 1685: 4388,\n",
       " 1766: 3216,\n",
       " 1877: 9655,\n",
       " 1916: 1933,\n",
       " 354: 2103,\n",
       " 610: 444,\n",
       " 1234: 2866,\n",
       " 1414: 2395,\n",
       " 1448: 1422,\n",
       " 1653: 5253,\n",
       " 1695: 657,\n",
       " 1729: 1712,\n",
       " 1775: 1433,\n",
       " 1819: 1694,\n",
       " 1838: 3541,\n",
       " 2029: 3977,\n",
       " 37: 1212,\n",
       " 134: 408,\n",
       " 242: 758,\n",
       " 399: 7109,\n",
       " 641: 1565,\n",
       " 1446: 1653,\n",
       " 1493: 1083,\n",
       " 1582: 922,\n",
       " 1671: 1255,\n",
       " 2007: 2065,\n",
       " 2016: 1319,\n",
       " 2027: 1767,\n",
       " 672: 859,\n",
       " 707: 1308,\n",
       " 723: 1779,\n",
       " 876: 2437,\n",
       " 1132: 5453,\n",
       " 1378: 2049,\n",
       " 1480: 2065,\n",
       " 1560: 5641,\n",
       " 1660: 1016,\n",
       " 1801: 1427,\n",
       " 100: 464,\n",
       " 360: 1713,\n",
       " 1139: 277,\n",
       " 1591: 3233,\n",
       " 1843: 1106,\n",
       " 1896: 1693,\n",
       " 1956: 2234,\n",
       " 865: 1399,\n",
       " 1275: 685,\n",
       " 1902: 694,\n",
       " 0: 1478,\n",
       " 925: 2789,\n",
       " 1119: 2680,\n",
       " 1127: 511,\n",
       " 1383: 497,\n",
       " 1482: 823,\n",
       " 1643: 1803,\n",
       " 228: 4685,\n",
       " 492: 1070,\n",
       " 894: 129,\n",
       " 1089: 3120,\n",
       " 1232: 2307,\n",
       " 1282: 654,\n",
       " 1742: 1281,\n",
       " 1935: 486,\n",
       " 740: 1863,\n",
       " 1047: 1260,\n",
       " 1830: 1585,\n",
       " 281: 830,\n",
       " 798: 1303,\n",
       " 839: 974,\n",
       " 1413: 913,\n",
       " 1473: 1309,\n",
       " 1562: 1188,\n",
       " 1564: 275,\n",
       " 1579: 3244,\n",
       " 1612: 261,\n",
       " 1862: 375,\n",
       " 1912: 361,\n",
       " 1925: 925,\n",
       " 1126: 1331,\n",
       " 1168: 3656,\n",
       " 102: 182,\n",
       " 302: 3506,\n",
       " 653: 274,\n",
       " 657: 1522,\n",
       " 1205: 5510,\n",
       " 1442: 3242,\n",
       " 1481: 3153,\n",
       " 1675: 2010,\n",
       " 988: 288,\n",
       " 1329: 589,\n",
       " 1782: 384,\n",
       " 191: 792,\n",
       " 308: 1112,\n",
       " 1158: 255,\n",
       " 1401: 464,\n",
       " 2000: 1134,\n",
       " 149: 703,\n",
       " 666: 1264,\n",
       " 1227: 670,\n",
       " 1698: 1265,\n",
       " 1576: 195,\n",
       " 177: 459,\n",
       " 908: 645,\n",
       " 996: 968,\n",
       " 1622: 582,\n",
       " 329: 365,\n",
       " 458: 3650,\n",
       " 490: 572,\n",
       " 668: 3350,\n",
       " 742: 1732,\n",
       " 1181: 70,\n",
       " 1237: 426,\n",
       " 1266: 411,\n",
       " 1355: 122,\n",
       " 1566: 371,\n",
       " 920: 401,\n",
       " 1397: 833,\n",
       " 856: 1047,\n",
       " 1189: 917,\n",
       " 174: 245,\n",
       " 321: 1148,\n",
       " 862: 436,\n",
       " 1751: 1317,\n",
       " 1345: 526,\n",
       " 1841: 993,\n",
       " 592: 91,\n",
       " 695: 67,\n",
       " 980: 377,\n",
       " 1435: 383,\n",
       " 1641: 43,\n",
       " 1888: 1670,\n",
       " 949: 359,\n",
       " 1121: 563,\n",
       " 1305: 843,\n",
       " 1024: 1952,\n",
       " 1973: 97,\n",
       " 944: 37849,\n",
       " 1425: 1575,\n",
       " 1406: 2834,\n",
       " 1022: 263,\n",
       " 1640: 932,\n",
       " 1696: 749,\n",
       " 68: 507,\n",
       " 364: 371,\n",
       " 349: 225,\n",
       " 394: 191,\n",
       " 493: 1170,\n",
       " 1484: 2142,\n",
       " 1664: 284,\n",
       " 1929: 2015,\n",
       " 1999: 3498,\n",
       " 517: 151,\n",
       " 841: 401,\n",
       " 1248: 234,\n",
       " 1450: 1063,\n",
       " 1520: 784,\n",
       " 282: 667,\n",
       " 629: 174,\n",
       " 904: 784,\n",
       " 1637: 1306,\n",
       " 1989: 676,\n",
       " 2037: 493,\n",
       " 682: 239,\n",
       " 84: 35,\n",
       " 337: 103,\n",
       " 686: 17,\n",
       " 1145: 1497,\n",
       " 154: 171,\n",
       " 268: 969,\n",
       " 304: 85,\n",
       " 310: 511,\n",
       " 700: 115,\n",
       " 782: 153,\n",
       " 957: 294,\n",
       " 1101: 662,\n",
       " 1142: 217,\n",
       " 1755: 478,\n",
       " 1906: 901,\n",
       " 1970: 895,\n",
       " 2047: 227,\n",
       " 1312: 613,\n",
       " 1747: 375,\n",
       " 135: 165,\n",
       " 964: 243,\n",
       " 1519: 124,\n",
       " 1811: 161,\n",
       " 857: 2897,\n",
       " 1890: 327,\n",
       " 885: 2338,\n",
       " 1206: 78,\n",
       " 1317: 198,\n",
       " 709: 784,\n",
       " 113: 20,\n",
       " 948: 1069,\n",
       " 1110: 1101,\n",
       " 1270: 613,\n",
       " 1512: 2251,\n",
       " 1602: 279,\n",
       " 148: 626,\n",
       " 1814: 114,\n",
       " 1474: 137,\n",
       " 464: 1267,\n",
       " 1674: 111,\n",
       " 1909: 238,\n",
       " 365: 213,\n",
       " 421: 659,\n",
       " 1769: 2048,\n",
       " 1403: 35,\n",
       " 119: 134,\n",
       " 1527: 6,\n",
       " 1880: 281,\n",
       " 2003: 42,\n",
       " 2041: 306,\n",
       " 1733: 127,\n",
       " 1867: 192,\n",
       " 1030: 389,\n",
       " 1457: 315,\n",
       " 225: 113,\n",
       " 1644: 625,\n",
       " 1804: 381,\n",
       " 993: 2475,\n",
       " 1116: 428,\n",
       " 1352: 876,\n",
       " 817: 524,\n",
       " 848: 274,\n",
       " 1913: 801,\n",
       " 2006: 408,\n",
       " 2008: 520,\n",
       " 155: 682,\n",
       " 622: 535,\n",
       " 1760: 190,\n",
       " 1495: 292,\n",
       " 146: 80,\n",
       " 1533: 87,\n",
       " 87: 74417,\n",
       " 810: 130,\n",
       " 1144: 494,\n",
       " 1750: 75,\n",
       " 1276: 1293,\n",
       " 1438: 3940,\n",
       " 1559: 245,\n",
       " 1777: 216,\n",
       " 491: 310,\n",
       " 1703: 8,\n",
       " 1424: 437,\n",
       " 1599: 785,\n",
       " 775: 320,\n",
       " 1543: 123,\n",
       " 1749: 1148,\n",
       " 244: 233,\n",
       " 1596: 5,\n",
       " 1917: 443,\n",
       " 444: 7,\n",
       " 1471: 1987,\n",
       " 1580: 519,\n",
       " 374: 5,\n",
       " 1372: 295,\n",
       " 378: 331,\n",
       " 383: 239,\n",
       " 846: 127,\n",
       " 1233: 9,\n",
       " 1537: 25,\n",
       " 1686: 195,\n",
       " 1796: 54,\n",
       " 1798: 88,\n",
       " 1465: 65,\n",
       " 1694: 189,\n",
       " 214: 269,\n",
       " 97: 185,\n",
       " 107: 281,\n",
       " 175: 26,\n",
       " 1499: 25,\n",
       " 1551: 35,\n",
       " 1592: 137,\n",
       " 1633: 116,\n",
       " 1669: 83,\n",
       " 1886: 647,\n",
       " 128: 137,\n",
       " 1692: 88,\n",
       " 1926: 826,\n",
       " 10: 52,\n",
       " 606: 200,\n",
       " 64: 352,\n",
       " 1290: 201,\n",
       " 1656: 36,\n",
       " 1765: 160,\n",
       " 794: 162,\n",
       " 198: 35,\n",
       " 1793: 244,\n",
       " 675: 106,\n",
       " 1954: 155,\n",
       " 504: 100,\n",
       " 1904: 126,\n",
       " 1040: 32,\n",
       " 213: 294,\n",
       " 1207: 57,\n",
       " 603: 92,\n",
       " 1786: 72,\n",
       " 1949: 5,\n",
       " 260: 60,\n",
       " 433: 16,\n",
       " 1293: 24,\n",
       " 1683: 94,\n",
       " 157: 63,\n",
       " 1221: 444,\n",
       " 1346: 369,\n",
       " 1588: 51,\n",
       " 1941: 56,\n",
       " 1300: 308,\n",
       " 1039: 14,\n",
       " 1774: 176,\n",
       " 1154: 18,\n",
       " 1398: 27,\n",
       " 1475: 9,\n",
       " 1587: 26,\n",
       " 702: 39,\n",
       " 145: 82,\n",
       " 47: 125,\n",
       " 837: 100,\n",
       " 1245: 87,\n",
       " 1514: 69,\n",
       " 1195: 151,\n",
       " 1680: 864,\n",
       " 1467: 63,\n",
       " 1210: 264,\n",
       " 721: 18,\n",
       " 1860: 17,\n",
       " 347: 67,\n",
       " 632: 193,\n",
       " 1322: 53,\n",
       " 272: 20,\n",
       " 352: 21,\n",
       " 1412: 43,\n",
       " 1997: 13,\n",
       " 516: 67,\n",
       " 736: 265,\n",
       " 781: 40,\n",
       " 898: 112,\n",
       " 1235: 108,\n",
       " 1944: 80,\n",
       " 2032: 11,\n",
       " 795: 62,\n",
       " 74: 681,\n",
       " 1509: 77,\n",
       " 45: 360,\n",
       " 53: 10,\n",
       " 779: 327,\n",
       " 1043: 50,\n",
       " 1836: 100,\n",
       " 1396: 48,\n",
       " 1874: 73,\n",
       " 307: 93,\n",
       " 1965: 13,\n",
       " 1211: 203,\n",
       " 227: 220,\n",
       " 326: 1,\n",
       " 1708: 1,\n",
       " 1169: 1433,\n",
       " 1135: 79,\n",
       " 1236: 97,\n",
       " 1468: 53,\n",
       " 974: 55,\n",
       " 1937: 189,\n",
       " 1690: 253,\n",
       " 1488: 40,\n",
       " 212: 113,\n",
       " 1122: 18,\n",
       " 1428: 50,\n",
       " 1569: 2,\n",
       " 1883: 35,\n",
       " 361: 147,\n",
       " 827: 14,\n",
       " 1648: 21,\n",
       " 1712: 24,\n",
       " 1421: 8,\n",
       " 255: 2652,\n",
       " 1615: 62,\n",
       " 1665: 187,\n",
       " 744: 6,\n",
       " 298: 35,\n",
       " 124: 37,\n",
       " 1379: 118,\n",
       " 1808: 50,\n",
       " 812: 210,\n",
       " 671: 19,\n",
       " 1028: 24,\n",
       " 713: 3,\n",
       " 318: 46,\n",
       " 635: 45,\n",
       " 527: 6,\n",
       " 320: 72,\n",
       " 1487: 29,\n",
       " 1818: 32,\n",
       " 661: 38,\n",
       " 1844: 287,\n",
       " 1574: 48,\n",
       " 317: 25,\n",
       " 196: 20,\n",
       " 636: 1421,\n",
       " 1805: 13,\n",
       " 1380: 35,\n",
       " 29: 13,\n",
       " 718: 75,\n",
       " 30: 3,\n",
       " 207: 16,\n",
       " 697: 9,\n",
       " 760: 1,\n",
       " 788: 31,\n",
       " 1196: 42,\n",
       " 2002: 6,\n",
       " 165: 80,\n",
       " 1504: 37,\n",
       " 946: 49,\n",
       " 1440: 17,\n",
       " 1981: 25,\n",
       " 1031: 324,\n",
       " 1092: 33,\n",
       " 2009: 25,\n",
       " 786: 5,\n",
       " 608: 18,\n",
       " 1501: 18,\n",
       " 1477: 9,\n",
       " 1017: 4,\n",
       " 1498: 178,\n",
       " ...}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "keys must be str, int, float, bool or None, not int64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaestro_token_distribution.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 3\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(dict_list, f)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/json/__init__.py:179\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(skipkeys\u001b[38;5;241m=\u001b[39mskipkeys, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii,\n\u001b[1;32m    174\u001b[0m         check_circular\u001b[38;5;241m=\u001b[39mcheck_circular, allow_nan\u001b[38;5;241m=\u001b[39mallow_nan, indent\u001b[38;5;241m=\u001b[39mindent,\n\u001b[1;32m    175\u001b[0m         separators\u001b[38;5;241m=\u001b[39mseparators,\n\u001b[1;32m    176\u001b[0m         default\u001b[38;5;241m=\u001b[39mdefault, sort_keys\u001b[38;5;241m=\u001b[39msort_keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\u001b[38;5;241m.\u001b[39miterencode(obj)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m    180\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(chunk)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/json/encoder.py:430\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m _floatstr(o)\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 430\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/json/encoder.py:326\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_list\u001b[0;34m(lst, _current_indent_level)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 326\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    328\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_hf/lib/python3.12/json/encoder.py:377\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 377\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeys must be str, int, float, bool or None, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    378\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first:\n\u001b[1;32m    380\u001b[0m     first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: keys must be str, int, float, bool or None, not int64"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('maestro_token_distribution.json', 'w') as f:\n",
    "    json.dump(dict_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3347843"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(dict_list[0].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55695"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp = glob('/mnt/gestalt/home/lonian/datasets/mtg_crop_sep/*/*.mp3')\n",
    "len(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "464.125"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "55695*30/60/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/gestalt/home/lonian/datasets/mtg_crop_sep_token/1173417.npy\n",
      "/mnt/gestalt/home/lonian/datasets/mtg_crop_sep_token/521029.npy\n",
      "/mnt/gestalt/home/lonian/datasets/mtg_crop_sep_token/1112339.npy\n",
      "/mnt/gestalt/home/lonian/datasets/mtg_crop_sep_token/1415740.npy\n",
      "/mnt/gestalt/home/lonian/datasets/mtg_crop_sep_token/944152.npy\n",
      "/mnt/gestalt/home/lonian/datasets/mtg_crop_sep_token/118965.npy\n",
      "/mnt/gestalt/home/lonian/datasets/mtg_crop_sep_token/429965.npy\n",
      "/mnt/gestalt/home/lonian/datasets/mtg_crop_sep_token/1274165.npy\n",
      "/mnt/gestalt/home/lonian/datasets/mtg_crop_sep_token/1364775.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "path = glob('/mnt/gestalt/home/lonian/datasets/mtg_crop_sep_token/*.npy')\n",
    "\n",
    "for p in path:\n",
    "    data = np.load(p, allow_pickle=True)\n",
    "    # print(data.shape)\n",
    "    if data.shape != (4, 1500):\n",
    "        print(p)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lonian/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/lonian/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n",
      "9.999316524962345e-05\n",
      "9.99726628670463e-05\n",
      "9.993849845741523e-05\n",
      "9.989068136093872e-05\n",
      "9.982922465033349e-05\n",
      "9.975414512725057e-05\n",
      "9.966546331768191e-05\n",
      "9.956320346634876e-05\n",
      "9.944739353007342e-05\n",
      "9.93180651701361e-05\n",
      "9.917525374361912e-05\n",
      "9.901899829374047e-05\n",
      "9.884934153917997e-05\n",
      "9.86663298624003e-05\n",
      "9.847001329696653e-05\n",
      "9.826044551386744e-05\n",
      "9.803768380684242e-05\n",
      "9.780178907671789e-05\n",
      "9.755282581475769e-05\n",
      "9.729086208503172e-05\n",
      "9.701596950580806e-05\n",
      "9.672822322997304e-05\n",
      "9.642770192448535e-05\n",
      "9.611448774886923e-05\n",
      "9.578866633275286e-05\n",
      "9.545032675245811e-05\n",
      "9.509956150664795e-05\n",
      "9.473646649103816e-05\n",
      "9.436114097218058e-05\n",
      "9.397368756032445e-05\n",
      "9.357421218136385e-05\n",
      "9.316282404787868e-05\n",
      "9.273963562927694e-05\n",
      "9.230476262104675e-05\n",
      "9.185832391312641e-05\n",
      "9.140044155740098e-05\n",
      "9.09312407343346e-05\n",
      "9.045084971874735e-05\n",
      "8.995939984474622e-05\n",
      "8.945702546981967e-05\n",
      "8.894386393810562e-05\n",
      "8.842005554284295e-05\n",
      "8.788574348801673e-05\n",
      "8.734107384920769e-05\n",
      "8.678619553365659e-05\n",
      "8.622126023955445e-05\n",
      "8.564642241456985e-05\n",
      "8.50618392136244e-05\n",
      "8.446767045592827e-05\n",
      "8.386407858128705e-05\n",
      "8.32512286056924e-05\n",
      "8.262928807620842e-05\n",
      "8.199842702516581e-05\n",
      "8.135881792367684e-05\n",
      "8.07106356344834e-05\n",
      "8.005405736415125e-05\n",
      "7.938926261462366e-05\n",
      "7.871643313414717e-05\n",
      "7.803575286758363e-05\n",
      "7.734740790612135e-05\n",
      "7.665158643639968e-05\n",
      "7.594847868906076e-05\n",
      "7.523827688674219e-05\n",
      "7.45211751915254e-05\n",
      "7.37973696518537e-05\n",
      "7.306705814893441e-05\n",
      "7.233044034264034e-05\n",
      "7.158771761692464e-05\n",
      "7.083909302476452e-05\n",
      "7.008477123264848e-05\n",
      "6.932495846462261e-05\n",
      "6.855986244591104e-05\n",
      "6.778969234612584e-05\n",
      "6.701465872208215e-05\n",
      "6.623497346023419e-05\n",
      "6.545084971874738e-05\n",
      "6.466250186922326e-05\n",
      "6.387014543809226e-05\n",
      "6.3073997047691e-05\n",
      "6.227427435703997e-05\n",
      "6.147119600233759e-05\n",
      "6.0664981537187364e-05\n",
      "5.985585137257402e-05\n",
      "5.90440267166055e-05\n",
      "5.8229729514036705e-05\n",
      "5.74131823855921e-05\n",
      "5.6594608567103456e-05\n",
      "5.577423184847932e-05\n",
      "5.4952276512523136e-05\n",
      "5.4128967273616625e-05\n",
      "5.3304529216284974e-05\n",
      "5.247918773366113e-05\n",
      "5.1653168465865425e-05\n",
      "5.082669723831792e-05\n",
      "5e-05\n",
      "4.917330276168208e-05\n",
      "4.83468315341346e-05\n",
      "4.752081226633889e-05\n",
      "4.6695470783715024e-05\n",
      "4.587103272638339e-05\n",
      "4.504772348747687e-05\n",
      "4.422576815152071e-05\n",
      "4.340539143289656e-05\n",
      "4.2586817614407895e-05\n",
      "4.177027048596331e-05\n",
      "4.095597328339451e-05\n",
      "4.014414862742599e-05\n",
      "3.933501846281266e-05\n",
      "3.8528803997662433e-05\n",
      "3.772572564296004e-05\n",
      "3.692600295230901e-05\n",
      "3.612985456190778e-05\n",
      "3.5337498130776764e-05\n",
      "3.454915028125264e-05\n",
      "3.376502653976583e-05\n",
      "3.298534127791784e-05\n",
      "3.2210307653874176e-05\n",
      "3.144013755408896e-05\n",
      "3.0675041535377385e-05\n",
      "2.9915228767351532e-05\n",
      "2.9160906975235487e-05\n",
      "2.841228238307536e-05\n",
      "2.7669559657359676e-05\n",
      "2.6932941851065617e-05\n",
      "2.620263034814632e-05\n",
      "2.547882480847459e-05\n",
      "2.476172311325781e-05\n",
      "2.4051521310939238e-05\n",
      "2.334841356360032e-05\n",
      "2.2652592093878663e-05\n",
      "2.1964247132416353e-05\n",
      "2.128356686585282e-05\n",
      "2.061073738537635e-05\n",
      "1.9945942635848748e-05\n",
      "1.928936436551661e-05\n",
      "1.864118207632315e-05\n",
      "1.800157297483417e-05\n",
      "1.7370711923791567e-05\n",
      "1.67487713943076e-05\n",
      "1.613592141871296e-05\n",
      "1.553232954407171e-05\n",
      "1.4938160786375572e-05\n",
      "1.4353577585430152e-05\n",
      "1.3778739760445554e-05\n",
      "1.3213804466343421e-05\n",
      "1.2658926150792322e-05\n",
      "1.2114256511983264e-05\n",
      "1.1579944457157046e-05\n",
      "1.1056136061894386e-05\n",
      "1.0542974530180328e-05\n",
      "1.0040600155253767e-05\n",
      "9.549150281252635e-06\n",
      "9.068759265665385e-06\n",
      "8.599558442599001e-06\n",
      "8.141676086873576e-06\n",
      "7.695237378953237e-06\n",
      "7.260364370723062e-06\n",
      "6.8371759521213065e-06\n",
      "6.425787818636143e-06\n",
      "6.026312439675553e-06\n",
      "5.63885902781941e-06\n",
      "5.263533508961827e-06\n",
      "4.900438493352061e-06\n",
      "4.549673247541886e-06\n",
      "4.211333667247136e-06\n",
      "3.885512251130763e-06\n",
      "3.5722980755146407e-06\n",
      "3.271776770026952e-06\n",
      "2.984030494191942e-06\n",
      "2.7091379149682685e-06\n",
      "2.4471741852423237e-06\n",
      "2.198210923282118e-06\n",
      "1.9623161931575985e-06\n",
      "1.739554486132572e-06\n",
      "1.5299867030334814e-06\n",
      "1.333670137599713e-06\n",
      "1.1506584608200365e-06\n",
      "9.81001706259532e-07\n",
      "8.247462563808815e-07\n",
      "6.819348298638837e-07\n",
      "5.526064699265752e-07\n",
      "4.3679653365124013e-07\n",
      "3.3453668231809275e-07\n",
      "2.458548727494291e-07\n",
      "1.707753496665076e-07\n",
      "1.0931863906127323e-07\n",
      "6.150154258475757e-08\n",
      "2.733713295369754e-08\n",
      "6.834750376549789e-09\n",
      "0.0\n",
      "6.834750376549792e-09\n",
      "2.7337132953697554e-08\n",
      "6.150154258476315e-08\n",
      "1.0931863906127327e-07\n",
      "1.7077534966650766e-07\n",
      "2.458548727494292e-07\n",
      "3.3453668231808735e-07\n",
      "4.3679653365124035e-07\n",
      "5.526064699265811e-07\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "EPOCH=200\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "model = resnet18(pretrained=True)\n",
    "optimizer = AdamW(  model.parameters(), \n",
    "                        lr = 1e-4, \n",
    "                        weight_decay = 0.005, \n",
    "                        betas = (0.9, 0.999)    )\n",
    "scheduler = CosineAnnealingLR(  optimizer, \n",
    "                                T_max = EPOCH-10 )\n",
    "\n",
    "for i in range(EPOCH):\n",
    "    print(optimizer.param_groups[0]['lr'])\n",
    "    optimizer.step()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lonian/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/lonian/anaconda3/envs/mamba_hf/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0\n",
      "1 5.000000000000001e-07\n",
      "2 1.0000000000000002e-06\n",
      "3 1.5e-06\n",
      "4 2.0000000000000003e-06\n",
      "5 2.5e-06\n",
      "6 3e-06\n",
      "7 3.5e-06\n",
      "8 4.000000000000001e-06\n",
      "9 4.5e-06\n",
      "10 5e-06\n",
      "11 5.500000000000001e-06\n",
      "12 6e-06\n",
      "13 6.5000000000000004e-06\n",
      "14 7e-06\n",
      "15 7.500000000000001e-06\n",
      "16 8.000000000000001e-06\n",
      "17 8.5e-06\n",
      "18 9e-06\n",
      "19 9.5e-06\n",
      "20 1e-05\n",
      "21 9.999242283403047e-06\n",
      "22 9.996969364420003e-06\n",
      "23 9.993181935404006e-06\n",
      "24 9.987881150042625e-06\n",
      "25 9.981068623006435e-06\n",
      "26 9.97274642945716e-06\n",
      "27 9.962917104415578e-06\n",
      "28 9.951583641989313e-06\n",
      "29 9.938749494460812e-06\n",
      "30 9.924418571235737e-06\n",
      "31 9.90859523765213e-06\n",
      "32 9.891284313650684e-06\n",
      "33 9.872491072306547e-06\n",
      "34 9.852221238223083e-06\n",
      "35 9.830480985788115e-06\n",
      "36 9.807276937293138e-06\n",
      "37 9.782616160916103e-06\n",
      "38 9.75650616856839e-06\n",
      "39 9.728954913606603e-06\n",
      "40 9.699970788409895e-06\n",
      "41 9.669562621823579e-06\n",
      "42 9.637739676469769e-06\n",
      "43 9.604511645925893e-06\n",
      "44 9.56988865177194e-06\n",
      "45 9.533881240507335e-06\n",
      "46 9.496500380338356e-06\n",
      "47 9.45775745783713e-06\n",
      "48 9.417664274473162e-06\n",
      "49 9.376233043018495e-06\n",
      "50 9.333476383827585e-06\n",
      "51 9.28940732099301e-06\n",
      "52 9.24403927837822e-06\n",
      "53 9.197386075528486e-06\n",
      "54 9.149461923461334e-06\n",
      "55 9.100281420337736e-06\n",
      "56 9.049859547015364e-06\n",
      "57 8.998211662485283e-06\n",
      "58 8.94535349919344e-06\n",
      "59 8.891301158248431e-06\n",
      "60 8.836071104516916e-06\n",
      "61 8.779680161608291e-06\n",
      "62 8.722145506750038e-06\n",
      "63 8.663484665555373e-06\n",
      "64 8.60371550668479e-06\n",
      "65 8.542856236403074e-06\n",
      "66 8.480925393033512e-06\n",
      "67 8.41794184131093e-06\n",
      "68 8.353924766635321e-06\n",
      "69 8.288893669227774e-06\n",
      "70 8.222868358190533e-06\n",
      "71 8.155868945472943e-06\n",
      "72 8.087915839745149e-06\n",
      "73 8.019029740181442e-06\n",
      "74 7.949231630155055e-06\n",
      "75 7.878542770846454e-06\n",
      "76 7.806984694766967e-06\n",
      "77 7.734579199199762e-06\n",
      "78 7.661348339560194e-06\n",
      "79 7.58731442267752e-06\n",
      "80 7.5125000000000005e-06\n",
      "81 7.436927860725529e-06\n",
      "82 7.3606210248598065e-06\n",
      "83 7.283602736204245e-06\n",
      "84 7.20589645527566e-06\n",
      "85 7.12752585215998e-06\n",
      "86 7.0485147993021065e-06\n",
      "87 6.968887364234136e-06\n",
      "88 6.8886678022441645e-06\n",
      "89 6.807880548987869e-06\n",
      "90 6.7265502130452034e-06\n",
      "91 6.644701568424356e-06\n",
      "92 6.562359547015364e-06\n",
      "93 6.479549230995616e-06\n",
      "94 6.396295845189572e-06\n",
      "95 6.312624749385041e-06\n",
      "96 6.228561430608347e-06\n",
      "97 6.14413149536073e-06\n",
      "98 6.059360661818352e-06\n",
      "99 5.9742747519983115e-06\n",
      "100 5.8888996838929805e-06\n",
      "101 5.803261463575149e-06\n",
      "102 5.7173861772763265e-06\n",
      "103 5.631299983440609e-06\n",
      "104 5.545029104756577e-06\n",
      "105 5.4585998201696e-06\n",
      "106 5.372038456877023e-06\n",
      "107 5.285371382308646e-06\n",
      "108 5.198624996094944e-06\n",
      "109 5.111825722025486e-06\n",
      "110 5.025e-06\n",
      "111 4.938174277974515e-06\n",
      "112 4.85137500390506e-06\n",
      "113 4.764628617691355e-06\n",
      "114 4.677961543122977e-06\n",
      "115 4.5914001798304e-06\n",
      "116 4.5049708952434255e-06\n",
      "117 4.418700016559392e-06\n",
      "118 4.3326138227236755e-06\n",
      "119 4.246738536424851e-06\n",
      "120 4.161100316107022e-06\n",
      "121 4.07572524800169e-06\n",
      "122 3.9906393381816485e-06\n",
      "123 3.905868504639272e-06\n",
      "124 3.821438569391654e-06\n",
      "125 3.7373752506149593e-06\n",
      "126 3.6537041548104296e-06\n",
      "127 3.5704507690043855e-06\n",
      "128 3.4876404529846373e-06\n",
      "129 3.4052984315756467e-06\n",
      "130 3.323449786954798e-06\n",
      "131 3.2421194510121314e-06\n",
      "132 3.1613321977558375e-06\n",
      "133 3.0811126357658637e-06\n",
      "134 3.001485200697895e-06\n",
      "135 2.9224741478400215e-06\n",
      "136 2.84410354472434e-06\n",
      "137 2.7663972637957547e-06\n",
      "138 2.689378975140195e-06\n",
      "139 2.6130721392744733e-06\n",
      "140 2.537500000000001e-06\n",
      "141 2.4626855773224805e-06\n",
      "142 2.3886516604398063e-06\n",
      "143 2.3154208008002403e-06\n",
      "144 2.243015305233035e-06\n",
      "145 2.1714572291535472e-06\n",
      "146 2.100768369844947e-06\n",
      "147 2.0309702598185594e-06\n",
      "148 1.96208416025485e-06\n",
      "149 1.8941310545270597e-06\n",
      "150 1.827131641809467e-06\n",
      "151 1.761106330772227e-06\n",
      "152 1.6960752333646822e-06\n",
      "153 1.6320581586890707e-06\n",
      "154 1.569074606966488e-06\n",
      "155 1.5071437635969265e-06\n",
      "156 1.4462844933152115e-06\n",
      "157 1.386515334444627e-06\n",
      "158 1.3278544932499632e-06\n",
      "159 1.2703198383917092e-06\n",
      "160 1.213928895483085e-06\n",
      "161 1.15869884175157e-06\n",
      "162 1.1046465008065586e-06\n",
      "163 1.0517883375147194e-06\n",
      "164 1.000140452984637e-06\n",
      "165 9.497185796622652e-07\n",
      "166 9.00538076538668e-07\n",
      "167 8.52613924471516e-07\n",
      "168 8.059607216217809e-07\n",
      "169 7.605926790069916e-07\n",
      "170 7.165236161724174e-07\n",
      "171 6.737669569815061e-07\n",
      "172 6.323357255268383e-07\n",
      "173 5.922425421628703e-07\n",
      "174 5.534996196616451e-07\n",
      "175 5.161187594926666e-07\n",
      "176 4.801113482280602e-07\n",
      "177 4.4548835407410925e-07\n",
      "178 4.122603235302331e-07\n",
      "179 3.8043737817642193e-07\n",
      "180 3.500292115901061e-07\n",
      "181 3.2104508639339873e-07\n",
      "182 2.934938314316112e-07\n",
      "183 2.673838390838987e-07\n",
      "184 2.427230627068635e-07\n",
      "185 2.195190142118857e-07\n",
      "186 1.9777876177691754e-07\n",
      "187 1.7750892769345524e-07\n",
      "188 1.587156863493167e-07\n",
      "189 1.4140476234787174e-07\n",
      "190 1.255814287642651e-07\n",
      "191 1.1125050553919015e-07\n",
      "192 9.841635801068747e-08\n",
      "193 8.708289558442313e-08\n",
      "194 7.725357054284039e-08\n",
      "195 6.893137699356593e-08\n",
      "196 6.211884995737462e-08\n",
      "197 5.681806459599518e-08\n",
      "198 5.303063557999859e-08\n",
      "199 5.075771659695344e-08\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "EPOCH=200\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "model = resnet18(pretrained=True)\n",
    "optimizer = AdamW(  model.parameters(), \n",
    "                        lr = 1e-5, \n",
    "                        weight_decay = 0.005, \n",
    "                        betas = (0.9, 0.999)    )\n",
    "\n",
    "warm_up_iter = 20\n",
    "T_max = EPOCH\t# 周期\n",
    "lr_max = 1e-1\t# 最大值\n",
    "lr_min = 5e-4\t# 最小值\n",
    "\n",
    "# 为param_groups[0] (即model.layer2) 设置学习率调整规则 - Warm up + Cosine Anneal\n",
    "lambda0 = lambda cur_iter: cur_iter / warm_up_iter if  cur_iter < warm_up_iter else \\\n",
    "(lr_min + 0.5*(lr_max-lr_min)*(1.0+math.cos( (cur_iter-warm_up_iter)/(T_max-warm_up_iter)*math.pi)))/0.1\n",
    "\n",
    "# LambdaLR\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda0)\n",
    "for i in range(EPOCH):\n",
    "        print(i, optimizer.param_groups[0]['lr'])\n",
    "        optimizer.step()\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "print('{:03d}'.format(123))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba_hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
